\label{sec:chap5-pub-belief-MDP}
As we aware, recursive learning in many steps can be complex and expensive even in cooperation problem. By using public agent formulation, we can reduce the problem into single agent POMDP task, which would be easier to manage. This technique is proposed in \cite{nayyar2013decentralized} and extended to scale over by approximation and neural network in \cite{foerster2018bayesian}. 

The public belief MDP is based on the fact that every agent can some how infer the underlying state given a public observation (can also be an action too). After forming such a belief, each agent can executes and action that condition on its private observation and the belief over the state. Given a decoupled process, we can have a centralized agent that only infer the underlying state and distribute the common belief to each lower level agent to execute its action. The process can be summarized as follows:
\begin{itemize}
    \item The higher-level agent observers a public observation and construct a message $m_t \sim \pi^{\pri}_t(\cdot | o^{\pub}_t, a^{\pub}_t)$ with state distribution $\mathcal{B}_t = P(s_t | o^{\pub}_{\le t})$ where $o^{\pub}_{\le t}$ is the history of public policy till time $t$
    \item The lower-level agent after receives the message from the higher-level agent will execute its action based on its private observation, the message given, and belief over state calculated using higher-level agent i.e $a^i_t \sim \pi^i(\cdot | m_t, \mathcal{B}_t, o^{\pri}_t)$. 
\end{itemize}
Please note that by having a centralized public belief agent makes the coordination problem much easier since the each agent can reason about the other's action based on its the belief of other agent's private observation conditioned on message $m_t$. And, both of the agent and its higher-level counter part will try to optimize the same reward. Note that we can train the public agent individually for each agent however we have to make sure that they are the same, which can be implemented by a public seed \cite{foerster2018bayesian}.