\label{chapter:chap9}
\begin{miniabstract}
Another crucial extension to multi-agent policy would it its ability to infer the hidden state via its local information. We will consider the cooperative case in which the agent's action is available as part of \correctquote{public observation}. The main observation from \cite{nayyar2013decentralized, foerster2018bayesian} is that we can turn partially observable multi-agent problem into single agent POMDP by consider the public agent that forms a belief of the state given histories of public observation and given its belief over state, the public agent constructs a placeholder policy that acts according to its individual observation. By having the same placeholder policy for all agents, we ensure that all the agents are coordinated correctly. We will start by introducing Public Belief MDP framework. Then we introduce Variational Sequential Monte Carlo (VSMC) \cite{le2017auto, maddison2017filtering, naesseth2017variational}, which is a basis to Deep Variational Reinforcement Learning (DVRL) \cite{igl2018deep} - an algorithm that captures the belief over state. We will follows \cite{shvechikovjoint} on the control as probabilistic framework interpretation of DVRL to construct our multi-agent algorithms.
\end{miniabstract}

\section{Public Belief MDP}
\input{chapters/chapter09/contents/01_pub_belief_MDP}

\section{Variational Sequential Monte Carlo and Deep Variational Reinforcement Learning}
\input{chapters/chapter09/contents/02_VSMC_DVL}

\section{Probabilistic Public Agent}
\input{chapters/chapter09/contents/03_Prob_Pub_Belief}

\section{Implementation}
\input{chapters/chapter09/contents/04_Implementation}
