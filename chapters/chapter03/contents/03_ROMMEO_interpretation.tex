\label{sec:chap3-ROMMEO-interpretation}

\subsection{Understanding ROMMEO}
Before we examine the results, let's state some of the theoretical results for ROMMEO, which are discussed in \cite{tian2019regularized}. The authors showed that the Bellman equation in equation \ref{eqn:chap3-final-ROMMEO} is a contraction mapping, which shows that there exists a fixed point of agent and its opponent model action value function, while the updating agent and its opponent model does improve the action value. These results are analogous to results from section \ref{sec:chap2-derivation-soft-closed-form}. Thus ROMMEO is implemented via soft Actor-Critic like algorithm, see section \ref{sec:chap2-soft-ac-implement} and \cite{tian2019regularized} for more details. 

Let's examine the results that we get from the derivation of ROMMEO. We start with the definition of value function $V(s)$. Let's expand the meaning of it:
\begin{equation}
    V(s_{t}) =\mathbb{E}_{\pi_\theta, \rho_\phi}\brackb{Q(s_t, a^i_t, a^{-i}_t) - \log\frac{\rho_\phi(a^{-i}_t | s_t)}{P_\prior(a^{-i}_t | s_t)} - \log\frac{\pi_{\theta}(a^i_t | s_t, a^{-i}_t) }{P_{\prior}(a^i_t | s_t, a^{-i}_t)}}
\end{equation}
We can see that the Bellman equation holds for the following:
\begin{equation}
\begin{aligned}
\label{eqn:chap3-rollout-Q}
    Q(s_{t},& a^i_{t}, a^{-i}_{t}) = \beta r(s_t, a^i_t, a^{-i}_t) \\
    &+ \gamma\mathbb{E}_{s_{t+1}}\brackb{\mathbb{E}_{\pi_\theta, \rho_\phi}\brackb{Q(s_{t+1}, a^i_{t+1}, a^{-i}_{t+1}) - \log\frac{\rho_\phi(a^{-i}_{t+1} | s_{t+1})}{P_\prior(a^{-i}_{t+1} | s_{t+1})} - \log\frac{\pi_{\theta}(a^i_{t+1} | s_{t+1}, a^{-i}_{t+1}) }{P_{\prior}(a^i_{t+1} | s_{t+1}, a^{-i}_{t+1})}}}
\end{aligned}
\end{equation}
Please keep in mind that for the value function to be complete, we have to consider the regularizers for both policy and its opponent model, which is equivalent to the use of soft-max on action-value function, when we use the optional policies. Furthermore, the exciting part is how the agent's policy uses this function. Since we have expanded the equation, we can see that the value function is based on the expectation of \correctquote{optimal} opponent model\footnote{during training we don't have the exact form due to approximation error}, which means that the agent is also considering the optimality of opponent model implicitly. By reaching the fixed point\footnote{We don't think that it is the same as logistic stochastic best response equilibrium (LSBRE) that is proposed in \cite{yu2019multi} because we don't refer to any stationary distribution of any Markov chain. But they should be related somehow, and we left it for future work.}, we perfectly capture how the agent considers its opponent model to be optimal and how the opponent model considers an optimal agent, which can't be described probabilistically. One might wonder how does the agent $\pi_\theta(a^i_t | s_t, a^{-i}_t)$ executes its action ? According to \cite{tian2019regularized}, we can simply plug the value of opponent models so that the agent can return the action as we assume the optimality of the opponent model onward i.e 
\begin{equation}
\label{eqn:chap3-ROMMEO-execute-action}
\pi_\theta(a^i_t |s_t) = \int \pi_\theta(a^i_t | s_t, a^{-i}_t) \rho_\phi(a^{-i}_t | s_t) \dby a^{-i}_t 
\end{equation}
However, please note that the agent is best responding to its opponent model only and not the opponent itself (see the rollout or even ELBO in equation \ref{eqn:chap3-rollout-Q} and \ref{eqn:chap3-ROMMEO-ELBO} relatively). This might be trivial, but it can be a very subtle thing to miss. Let's view the value function from other forms, starting with the action-value function 
\begin{equation}
    Q(s_t, a^{-i}_t) = \log \int \exp \bracka{Q(s_t, a^i_t, a^{-i}_t}P_{\prior}(a^i_t | s_t, a^{-i}_t)  \dby a^i_t 
    % &V(s_t) = \log \int \exp\bracka{Q(s_t, a^{-i}_t)}P_{\prior}(a^{-i}_t | s_t) \dby a^{-i}_t
\end{equation}
This represents the action-value function for the opponent model as it only contains the state and opponent model's action. As mentioned before\footnote{in section \ref{sec:chap2-derivation-soft-closed-form}}, this resembles \correctquote{soft-max} of agent's action based on agent's prior. In other words, the opponent model's action-value function is the maximization of full action-value function that is also influenced by the opponent's\footnote{or opponent model i.e agent's belief that opponent will use $P_\prior (a^i_t | s_t, a^{-i}_t)$ as its (refer to opponent) prior for the agent} prior on agent policy. The opponent models doesn't simply use an expected value of the action value given agent's prior i.e $\mathbb{E}_{P_\prior (a^i_t | s_t, a^{-i}_t)}\brackb{Q(s_t, a^i_t, a^{-i}_t)}$ nor uses the action that maximizes agent's value i,e $\max_{a^i_t} Q(s_t, a^i_t, a^{-i}_t)$, but a mixture of both - prior based best response, since we know that the agent will try to increase its expected reward (a maximize) while trying to acts similarly to its prior (expected value). This is crucial, since the prior is not only a regularizer for the agent, but it also represents the opponent's belief on what the agent should do. In conclusion, MAPI framework doesn't only best response to the opponent model that is estimated via supervised learning but also best responding to optimal opponent the agent aware of.

