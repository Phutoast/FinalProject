\label{sec:chap3-intro-MAPI}

\subsection{How does MAPI work ?}
All of the algorithms in MAPI frameworks are decentralized MARL algorithms. They are based on an opponent model but not merely as the supervised opponent model as fictitious play \cite{berger2007brown}, which the agent assumes its opponent to be stationary as it tries to update its belief about its opponent and best response to the model. MAPI agents, instead, trains its opponent model given its reward. Since we know the agent's opponent model, we have reduced multi-agent problem into a single-agent problem. During the training phase, we will assume that the action given by the real opponent comes from our assumed opponent model, and we trained accordingly. The difference between ROMMEO \cite{tian2019regularized} and PR2 \cite{wen2019probabilistic} happens when we define which component we optimize first. In ROMMEO case, we train the agent's policy first then train the opponent model, and PR2 vice versa. Thus, we can view PR2's opponent model $\pi(a^{-i} | s, a^i)$ as an opponent's policy doing ROMMEO like training on its opponent (the agent) model $\pi(a^i | s, a^{-i})$. 

A training algorithm that incorporates the learning of the opponent model with the learning of the agent's policy can be beneficial in a cooperative case since both agent's policy and the real opponent (not agent's opponent model) tries to optimize the same objective. However, this is also a weakness because if we have an opponent with drastically difference goal (for example in the case of a competitive game), the agent would still update its opponent model to act as if it is maximizing the agent's reward. The problem will be more apparent once we have understood the resulting of update in Balancing Q-learning see theorem \ref{thm:update-balance-opponent}. We proposed a solution that decouples the agent's training from the opponent's model. Furthermore, one of the tricks that can be deployed to make the opponent model similar to the real opponent is to use supervised training on the prior opponent model given state and opponent action as a training set. By having a supervised opponent model, we can use it as a prior in our opponent model optimization as deployed in \cite{tian2019regularized}.

\subsection{What is Optimal ?}
In MAPI case, which is decentralized training, we assume the optimality of the agent \textit{relative} to our knowledge of the opponent, which we assume to be in some degree optimal given the objective. If we are best responding to an optimal agent then we are likely to be optimal too\footnote{This is true in Nash equilibrium case by definition but we didn't assume it to be as so.}. Still, we have a fully trained opponent model, i.e. what could our opponent \textit{could train to be} given the assumptions and objective. Intuitively, by assuming the optimality of our opponent model, we can have a better guess of what its policy can be. This assumption is crucial, which will allow us to extends the framework into cover MAKL framework. We want to note that the assumption is used in \cite{tian2019regularized}. However, it lacks the observation that the opponent's model can be almost arbitrary\footnote{They assume that the opponent model has to be \textit{optimal}, which isn't always the case. We presume our opponent model to be optimal according to us doesn't mean it has to be.} and it doesn't have to follow the same objective as the agent, which we can show in the derivation of ROMMEO. In conclusion, MAPI is an enhanced probabilistic version of the fictitious play, where the opponent model is a fully trained policy. At the same time, the optimality is similar to single-agent reinforcement learning. 



