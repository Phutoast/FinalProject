\label{sec:chap3-intro-MAPI}

\subsection{How does MAPI work ?}

What is the goal of MAPI framework ? All of MAPI frameworks are decentralized MARL algorithms. They are based on opponent model but not simply as the supervised opponent model as fictitious play \cite{berger2007brown}, which the agent assumes its opponent to be stationary while it tries to update its belief about its opponent and best response to the model. MAPI agents, instead, create its own inner loop of learning in which we have a full knowledge of the opponent (since it is the agent's inner model) and train the agent to best response to the opponent in its inner model. The learning of opponent and the agent happens in one step; we have reduced multi-agent problem into single agent problem. During the training phase, we will assume that the action given by the real opponent comes from our assumed opponent model, and we trained accordingly. The difference between ROMMEO \cite{tian2019regularized} and PR2 \cite{wen2019probabilistic} happens when we define which component is trained first and which component is trained last. In ROMMEO case, we train agent's policy first then train the opponent model, and PR2 vice versa. Thus, we can view PR2's opponent model as opponent's policy doing ROMMEO like training on its opponent (the agent) model. 

Training which we can include the learning of the opponent's into learning of the agent's policy can be extremely useful in cooperative case as we can assume both agent's policy and the real opponent (not agent's opponent model) tries to optimize the same goal. However, this is also a weakness because if we have an opponent with drastically difference goal (i.e in the case of zero-sum game), the agent would still update its opponent model to act as if it is maximizing the agent's reward. This will be more clear once we have understand the full derivation of MAPI algorithms, while to solution can be relatively simple, which to decoupling the training of agent's policy from opponent's policy.

There are many other problem with this approach. Notably, how can we be able to have approximately similar opponent model to the real opponent. One of the solution is to use supervised training based opponent model to be the prior for our model, which is deployed in \cite{tian2019regularized}. This is possible since we are doing probabilistic inference. The harder problem would be to be able to reach certain consensus on which equilibrium to use, which is know generally as equilibrium selection. We will not consider such a problem here in this chapter but we believe that communication is needed to coordinate the agent into correct use of difference mode of actions, which will be considered in Chapter \ref{chapter:chap6} and Chapter \ref{chapter:chap9}.

\subsection{What is Optimal ?}
This MAPI case, which is decentralized training, we assume the optimality of the agent \textit{relative} to our knowledge of the opponent, which we assume to be in some degree optimal given the objective. We simply doesn't just have our supervised estimation on the opponent's policy but we have a fully trained opponent model i.e what could our opponent \textit{be} given the assumptions and objective. Intuitively, by assuming and aim toward the optimality of our opponent model, we can have a better guess of what its policy can be. This is crucial observation, which will allow us to extends the framework into cover MAKL framework. We would like to note that the assumption is assumed in \cite{tian2019regularized}, however, it lacks the observation that the opponent's model can be almost arbitrary\footnote{They assume that the opponent model has to be \textit{optimal}, which isn't always the case. We assume our opponent model to be optimal according to us doesn't mean it has to be.}. In conclusion, MAPI is an enhanced probabilistic version of fictitious play, which we can extends our opponent model to be a fully trained policy. 


