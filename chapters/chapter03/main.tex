\label{chapter:chap3}

\begin{miniabstract}
Now, we will survey the maximum entropy multi-agent reinforcement learning (MEMARL) framework, which requires us to examine more deeply into each algorithms in control as probabilistic inference framework (MAPI). By re-derive the one of the algorithms ROMMEO \cite{tian2019regularized}, we understand how current MAPI operates, and what is the meaning of optimality. The main aim is to gain more understanding for the algorithms that will be used as a basis to construct others algorithms in this thesis.
\end{miniabstract}


\section{Introduction to MAPI framework}
\input{chapters/chapter03/contents/01_intro}

\section{Derivation of ROMMEO}
\input{chapters/chapter03/contents/02_ROMMEO_derivations}

\section{Interpretation of ROMMEO}
\input{chapters/chapter03/contents/03_ROMMEO_interpretation}

\section{Solving Balancing-Q learning}
\input{chapters/chapter03/contents/04_Balancing_Q}

\section{Probabilistic Balancing-Q}
\input{chapters/chapter03/contents/05_Prob_Balancing_Q_impl}