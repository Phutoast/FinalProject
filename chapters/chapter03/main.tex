\label{chapter:chap3}

\begin{miniabstract}
Now, we will survey the maximum entropy multi-agent reinforcement learning (MEMARL) framework, where we examine more deeply into each algorithm in control as probabilistic inference framework (MAPI). By re-derive one of the algorithms ROMMEO \cite{tian2019regularized} in MAPI framework, we understand how current MAPI operates. The main aim is to gain more understanding of the algorithms that will be used as a basis to construct other algorithms in this thesis. We conclude this chapter with a derivation of Balance Q-learning \cite{grau2018balancing} and a probabilistic interpretation of it.
\end{miniabstract}


\section{Introduction to MAPI framework}
\input{chapters/chapter03/contents/01_intro}

\section{Derivation of ROMMEO}
\input{chapters/chapter03/contents/02_ROMMEO_derivations}

\section{Interpretation of ROMMEO}
\input{chapters/chapter03/contents/03_ROMMEO_interpretation}

\section{Solving Balancing-Q learning}
\input{chapters/chapter03/contents/04_Balancing_Q}

\section{Probabilistic Balancing-Q}
\input{chapters/chapter03/contents/05_Prob_Balancing_Q_impl}