\label{sec:chap6-summary}
We have shown the \textit{potential} of control as probabilistic inference framework in multi-agent reinforcement learning. We started by expanding the training capability of the current framework by reinterpreting Balancing Q-learning into a probabilistic framework. The reinterpretation allows us to create a training algorithm that can work in a competitive setting via probabilistic inference, see chapter \ref{chapter:chap3}. Then, we moved to expand the agent's capability so that it can perform temporal abstraction with communication via the use of soft-option framework \cite{igl2019multitask} in chapter \ref{chapter:chap4}. Finally, we concluded our work on reasoning in an unknown state via public-belief MDP framework, which arises almost naturally given the graphical model representation of the problem, see chapter \ref{chapter:chap5}. 

Given two examples, we have sufficiently showed that control as probability inference is \textit{one of the prominent ways} multi-agent reinforcement learning problem can be tackled systematically while being inherently complex. The algorithms derived from this framework not only have theoretical grounded support but also serves as a \correctquote{soft} counterpart to many algorithms that are derived before, for example, Delayed Soft-Option Communication algorithm is a direct counterpart to algorithm proposed in \cite{han2019multi}. We hope that this framework will be developed further and extended to many other sub-classes of problems in multi-agent reinforcement learning literature. 
