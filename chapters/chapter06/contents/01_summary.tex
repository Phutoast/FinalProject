\label{sec:chap6-summary}
Now, we have shown the potential of control as probabilistic inference framework in multi-agent reinforcement learning. We start by expanding the training capability of the current framework by reinterpreting Balancing-Q learning into a probabilistic framework. Then we move to expand agent's capability so that it can do temporal abstracting with communication and reasoning in an unknown state via public-belief MDP framework, which arises almost naturally given the graphical model representation of the problem. The ability of this framework lies in the fact that we can represent our problem as a graphical model, which leads directly to the answer. By utilizing tools from single-agent reinforcement, the algorithm developed is sounded. 

Finally, we show that control as probability inference is one of the prominent ways multi-agent reinforcement learning problem can be tackled systematically, which is inherently complex, via the understanding of the problem and interpreting it as a probabilistic model.