\label{chapter:chap6}
\begin{miniabstract}
After we have a unified view on MAPI framework, which we have reinterpret Balancing Q-learning \cite{grau2018balancing} as a probabilistic inference. We are now going to consider extensions to agent's functionality. In this chapter, we are going to consider the case of temporal abstraction and communication in coopeartive multi-agent reinforcement learning problem. We will start by considering single agent option learning problem and solving it together with soft actor-critic like algorithm, as a minor contribution. After considering single agent problem, we are going to define a model in which the agent can plan and communicate and solving then.
\end{miniabstract}


\section{Single Agent Soft-Hierachical Reinforcement Learning}
\input{chapters/chapter06/contents/01_single_agent_HRL}

\section{Multi-Agent Delayed Soft-Option Communication}
\input{chapters/chapter06/contents/02_multi_agent_comunication}

