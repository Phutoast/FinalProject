\label{sec:chap5-VSMC-DVL}

\subsection{VSMC}
Let's start with Importance-Weight Auto Encoder \cite{burda2015importance}. Consider the Jensen equality derivation of ELBO, where we can show that 
\begin{equation}
    \log P(X) = \log \bracka{\int P(X, z) \dby z}  = \log \bracka{\int P(z|X) P(X) \dby z} = \log \bracka{\mathbb{E}_{P(z | X)}\brackb{P(X)}}
\end{equation}
Now, given the expected value, we can use our variational distribution $q_\phi(z)$ to be our importance sampling distribution i.e
\begin{equation}
\begin{aligned}
    \log \mathbb{E}_{q_\phi(z |x)}\brackb{\frac{P(z | X)}{q_\phi(z |x)} P(X)} &= \log \mathbb{E}_{q_\phi(z |x)}\brackb{\frac{P(X, z)}{q_\phi(z |x)}} \\
    &= \log \mathbb{E}_{z_1, \dots, z_K}\brackb{\frac{1}{K}\sum^K_{k=1} \frac{P(X, z)}{q_\phi(z |x)}} \ge \mathbb{E}\brackb{\log \frac{1}{K}\sum^K_{k=1} \frac{P(X, z)}{q_\phi(z |x)}}
\end{aligned}
\end{equation}
We will now generalize the result from IWAE by using the notion of Monte Carlo objective \cite{maddison2017filtering}, where we define the denote positive estimator of $P(x)$ as $\hat{P}_N(x)$. Then, the Monte Carlo objective (which is always a lower bound of $P(x)$) is defined as 
\begin{equation}
\label{eqn:chap5-elbo-general}
    \mathbb{E}[\log \hat{p}_N(x)]
\end{equation}
For a hidden Markov model, we can simply calculate the $P(x_{1:T})$ where $x_t$ is an observable variable using Sequential Monte Carlo (SMC) \cite{doucet2001introduction} we can train variational emission distribution and transition distribution using a lower bound in equation \ref{eqn:chap5-elbo-general} via back-propagation.  

\subsection{DVRL}
Typically, in reinforcement learning, we usually use a recurrent neural network (RNN) to encode the state to $h_t$ condition of the past action and observation. Given the RNN state $h_t$ we can then condition our policy on $h_t$. However, this only works because of memory and heuristic from RNN. DVRL aim to solve POMDP by 
\begin{itemize}
    \item Estimating the transition and observation model 
    \item Perform an inference model 
    \item Choosing an action based on inferred belief state.
\end{itemize}
The authors uses 3 elements triplet $(h^k_t, z^k_t, w^k_t)$ where $h^k_t$ is the latent of RNN, $z^k_t$ is the additional stochastic latent state and $w^k_t$ is the weight, as a particle. The belief is the updated, at time $t$, as follows 
\begin{equation}
\begin{aligned}
    &u^k_{t-1} \sim \text{Categorical}\bracka{\frac{w^k_{t-1}}{\sum^K_{k=1}w^k_{t-1}}} \\
    &z^k_{t} \sim q_{\phi}(z^k_t | h^{u^k_{t-1}}_{t-1}, a_{t-1}, o_t) \\
    &h^k_t = \psi^{\text{RNN}}_\theta (h^{u^k_{t-1}}_{t-1}, z^k_t, a_{t-1}, o_t) \\
    &w^k_T = \frac{P_\theta(z^k_t | h^{u^k_{t-1}}_{t-1}) P_\theta(o_t | h^{u^k_{t-1}}_{t-1}, z^k_t, a_{t-1})}{q_\phi(z^k_t | h^{u^k_{t-1}}_{t-1}, a_{t-1}, o_t)}
\end{aligned}
\end{equation}
The model of the world is trained via ELBO 
\begin{equation}
    \mathcal{L}^{ELBO}_{t}(\theta, \phi) = -\frac{1}{n_en_s} \sum^{n_e}_{\text{envs}} \sum^{n_s - 1}_{i=0} \log \bracka{\frac{1}{K}\sum^K_{k=1}w^k_{t+i}}
\end{equation}
The agent executes its action $\pi(a_t | \hat{h}_t)$ \footnote{and calculate its value, since it is Actor-Critic model} by using another RNN to aggregate all the history of particles, which returns $\hat{h}_t$ . The agent's reward and its model of the world are trained \textit{jointly}f\footnote{We disagree with \cite{huangsvqn} when the authors claim that \cite{igl2018deep} \correctquote{separate the planning algorithm and the inference of the hidden state.}, since \cite{igl2018deep} claims in the abstract that DVRL \correctquote{allowing the model to be trained jointly with the policy.}} using back-propagation through time (BPTT) via standard Actor-Critic framework. For more information on BPTT training for the agent, we refer to the paper for more details. \cite{shvechikovjoint} has proposed a graphical model representation of the problem, while showing that a similar algorithm could be derived from control as inference framework. The major difference is how the agent's action is calculated. As mentioned before \cite{igl2018deep} uses secondary RNN to aggregate the histories to inform the agent, however, \cite{shvechikovjoint} uses a weighted average on the action of the agent over its belief over state i.e
\begin{equation}
    \sum^K_{k=1} \frac{w^k_{t}}{\sum^K_{k=1}w^k_{t}} \pi(a_t | h_t^k)
\end{equation}
Another work on using control as inference framework is \cite{huangsvqn}. The authors also consider optimizing ELBO objective. However, we believe that \cite{shvechikovjoint} provides more general solution since once the number of particle is equal to one, ELBO derived in \cite{huangsvqn}\footnote{See equation 19 in appendix of \cite{huangsvqn}} is similar to \cite{shvechikovjoint}\footnote{See equation 4 in appendix of \cite{shvechikovjoint}}/ELBO from multiple. Furthermore, by having multiple possibilities of hidden state with its weight, we can consider a counter-factual in multi-agent scenario, thus being more useful than one state representation. Finally, the differences between \cite{huangsvqn} and \cite{shvechikovjoint} lies in the use of VAE like objective when considering state transition, as \cite{huangsvqn} minimizes VAE objective for state representation as an auxiliary loss, while \cite{shvechikovjoint} doesn't and focuses on maximizing ELBO only.