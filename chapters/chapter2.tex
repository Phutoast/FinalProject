We will start by formalizing difference graphical models representing our assumptions on agents and their interaction. This will lead to ELBO representing methods in difference literature mainly:  \cite{tian2019regularized} and \cite{grau2018balancing}. Let's start with a one-step game, and extending to a stochastic game can be shown later.

\section{Literature Review}
\subsection{\cite{grau2018balancing} is \emph{almost} the answer}
ROMMEO \cite{tian2019regularized} is based on the fact that the game is fully-cooperative, and therefore, isn't working with the other side of the game spectrum -- zero-sum game. Looking carefully, this is due to its disability to model the perfect opponent to be an adversary. (We will explore this fact later in the section. \emph{\textbf{Spoiler:} We believe that a graphical model doesn't facilitate this}). 

Balancing Soft-Q \cite{grau2018balancing}, on the other hand, doesn't rely on an agent having a perfect opponent \emph{model}, instead, trains a perfect opponent as another \emph{player}. Conceptually, the ideas between ROMMEO and Balancing Soft-Q are the same: training multi-agent game assuming a perfect opponent. 
\begin{tcolorbox}
\textbf{Lesson 0: } ROMMEO creates an optimal opponent model within an agent's scope, although, this can also be replaced with the real opponent, that also trains to be optimal. The advantage of this is to make training decentralized. \Phu{Correct me if I am wrong}
\end{tcolorbox}
The biggest advantage of Balancing Soft-Q is due to its ability to represent both cooperative game and zero-sum game -- 2 ends of the spectrum. Let's take a look at the optimizing objective for a one-step game: 

\begin{equation}
    \mathbb{E}\Bigg[ R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) - \frac{1}{\beta_i} \log \frac{\pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})} - \frac{1}{\beta_{-i}} \log \frac{\rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s})}  \Bigg]
\end{equation}
We can then derive an optimal policy for both player and the opponent to be 
\begin{equation}
    \begin{aligned}
        \pi_{\theta}^*(\boldsymbol{a}^i | \boldsymbol{s}) &= \frac{1}{N_{i}(\boldsymbol{s})} P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}) \exp\left( \beta_i R(\boldsymbol{s}, \boldsymbol{a}^i) \right) \\
        \rho_{\phi}^*(\boldsymbol{a}^{-i} | \boldsymbol{s}) &= \frac{1}{N_{-i}(\boldsymbol{s})} P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}) \exp\left( \beta_{-i} R(\boldsymbol{s}, \boldsymbol{a}^{-i}) \right)
    \end{aligned}
\end{equation}
Throughout the derivation, we will assume $\beta_i$ to be positive. 
\Phu{Need Revision}
And from this, we can see that the agent's policy or its \emph{perception} of the reward, which is based on the value of $\beta$. For instance, If $\beta_{-i}$ is negative, the opponent reward would be shifted to be negative of game reward, marginalized by the agent's prior policy, hence creating a zero-sum scenario. Similarly, if $\beta_{-i}$ is a small number (either negative or positive), the reward wouldn't matter, and therefore the policy of the opponent is optimized to be near its prior. As described in \cite{grau2018balancing}, $\beta_{-i}$ indicates both the agent's rationality and its behaviour (team or zero-sum).

\begin{tcolorbox}
\textbf{Lesson 1: } $\beta$ is important because it can tell us almost everything about agents in the game, from its behaviour toward others to rationality. We will call it a \emph{characteristic variable}. 
\end{tcolorbox}
Although there is a way to characterized the notion of non-optimal, we believe that its definition is vague in both ends of the game spectrum, since non-optimal in team game can be either: near to prior (being uniform in this case) or being zero-sum, or not even both and similarly for zero-sum game. 

% Furthermore, its definition involves too many notion, instead, we will assign the optimality of each action, some are more optimal than the others. 

Even with the success of \cite{grau2018balancing}, there are still rooms for improvement and investigation, which we will discuss in the next section.

\subsection{Conceptual problem with \cite{grau2018balancing}}

The biggest problem with \cite{grau2018balancing} is with the steps it took to arrive at the objective. Firstly, let's see the optimizing objective for bounded rationality in the case of single-agent:

\begin{equation}
    \begin{aligned}
        &\pi_{\theta}^* = \arg\max_{\theta} \mathbb{E}\left[ R(\boldsymbol{s}, \boldsymbol{a}) \right] \\
        &\text{s.t } D_{KL}\left[ \pi_{\theta}(\boldsymbol{a} | \boldsymbol{s}) \Big|\Big| P_{\text{prior}}(\boldsymbol{a} | \boldsymbol{s}) \right] \le C
    \end{aligned}
\end{equation}
The objective can either be derived by introducing Lagrange multiplier $\beta$ or by probabilistic inference. For a probabilistic inference, please see \ref{fig:one-step-single agent}, for a graphical model. 
\begin{figure}[ht]
    \begin{minipage}[t]{0.5\linewidth}
    \centering
    \begin{tikzpicture}[latent/.append style={minimum size=1.0cm}]
        \node[obs] (o) {$\mathcal{O}^i$};
        \node[latent, below=of o] (a) {$\boldsymbol{a}^i$};
        \node[latent, below=of a] (s) {$\boldsymbol{s}$};
    
        \edge {s} {a}
        \edge {a} {o}
        \path[->]  (s)  edge   [bend left=30] node {} (o);
    \end{tikzpicture}
    \end{minipage}%
    \begin{minipage}[t]{0.5\linewidth}
    \captionof{figure}{Graphical model of One-Step Single agent reinforcement learning with visible optimality variable.} 
    \label{fig:one-step-single agent}
    \end{minipage}
\end{figure}
In this case, we can arrive at the same objective as a Lagrange multiplier method. However, for a multi-agent case, strictly speaking, having difference Lagrange multiplier for each of KL-Divergence term (the agent and the opponent) is \emph{impossible} (currently), if we derived the objective from this innocent looking graphical model (see \ref{fig:sad-graphical-balance-q}): 

\begin{figure}[ht]
    \begin{minipage}[t]{0.5\linewidth}
    \centering
    \begin{tikzpicture}[latent/.append style={minimum size=1.0cm}]
        \node[obs] (o) {$\mathcal{O}^i$};
        \node[latent, below=of o] (a) {$\boldsymbol{a}^i$};
        \node[latent, below=of a] (s) {$\boldsymbol{s}$};
        \node[latent, below=of s] (a1) {$\boldsymbol{a}^{-i}$};
        \node[obs, below=of a1] (o1) {$\mathcal{O}^{-i}$};
        
        \edge{o1} {a1}
        \edge {s} {a, a1}
        \edge {a} {o}
        \path[->]  (o1)  edge   [bend left=60] node {} (o);
        \path[->]  (a1)  edge   [bend left=45] node {} (o);
        \path[->]  (s)  edge   [bend left=30] node {} (o);
    \end{tikzpicture}
    \end{minipage}%
    \begin{minipage}[t]{0.5\linewidth}
    \captionof{figure}{Attempt to define Balancing Soft-Q learning via a graphical model. We can see that the optimality of the agent $\mathcal{O}^{i}$ is based on the optionally of other agents $\mathcal{O}^{-i}$. Furthermore, the action of opponent agents is assumed to be optimal.} 
    \label{fig:sad-graphical-balance-q}
    \end{minipage}
\end{figure}
Let's define the joint probability of this graphical model 
\begin{equation}
    \begin{aligned}
        P(\mathcal{O}^i = 1, \boldsymbol{a}^i, \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) = &P(\mathcal{O}^i = 1 | \boldsymbol{a}^i, \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) \\
        &P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}) P(\boldsymbol{s}) P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) P(\mathcal{O}^{-i} = 1)
    \end{aligned}
\end{equation}
Now define the variational distribution. 
\begin{equation}
    \begin{aligned}
        q(\boldsymbol{a}^i, \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) = \pi_{\theta} (\boldsymbol{a}^i | \boldsymbol{s}) \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) P(\boldsymbol{s}) P(\mathcal{O}^{-i} = 1)
    \end{aligned}
\end{equation}
Let the optimality of the agent be defined, given that the opponent is optimal, to be  
\begin{equation}
    P(\mathcal{O}^i = 1 | \boldsymbol{a}^i, \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) = \exp\Big( \beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \Big)
\end{equation}
Now we can see where the variable $\beta$ is coming from. With this, the ELBO can be found by minimizing KL-Divergence between a variational distribution and joint probability:
\begin{equation*}
    \begin{aligned}
        &\begin{aligned}[t]
            \mathbb{E}_{q(\boldsymbol{a}^i, \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1)} \bigg[ &\log \pi_{\theta} (\boldsymbol{a}^i | \boldsymbol{s}) + \log \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) + \cancel{\log P(\boldsymbol{s})} + \cancel{\log P(\mathcal{O}^{-i} = 1)}  \\
            &-\log \exp\Big( \beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \Big) - \log P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}) - \cancel{\log  P(\boldsymbol{s})} \\
            &- \log P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) -\log \cancel{P(\mathcal{O}^{-i} = 1)} \bigg]
        \end{aligned} \\
        &= \begin{aligned}[t]
            \mathbb{E}_{q(\boldsymbol{a}^i, \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1)} \bigg[ &- \beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) + \log \pi_{\theta} (\boldsymbol{a}^i | \boldsymbol{s}) + \log \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}) \\
            &-\log P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}) - \log P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) \bigg]
        \end{aligned}
    \end{aligned}
\end{equation*}
The optimizing objective is 
\begin{equation}
    \mathbb{E}_{q(\boldsymbol{a}^i, \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1)} \bigg[ R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) + \frac{1}{\beta} \log \frac{\pi_{\theta} (\boldsymbol{a}^i | \boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})}  + \frac{1}{\beta} \log \frac{\rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1)} \bigg]
\end{equation}
Superficially, the objective looks striking similar to Balancing Soft-Q objective, however, the nature of $\beta$ for each KL-divergence is different.  This leads to an opponent model that resembles the value viewed by the agent since $\beta$ are the same for both agents and opponent model. This leads to the main problem we are trying to solve: \textbf{How to inject the opponent's characteristic into our graphical model without relying on Lagrange multiplier ?}

\begin{tcolorbox}
\textbf{Lesson 2: } Opponent Characteristic can't be represented in centralized graphical model -- a graphical model that unifying every dependency from opponent model to agent's policy -- due to different characteristic of both agent's and opponent's. Balancing Soft-Q works because it contains separate Lagrange multipliers (and 2 stages of optimization)
\end{tcolorbox}

Furthermore, although Lagrange multiplier's method yields very interesting objective, we didn't see any connection between constraint on KL-Divergence to a behaviour of the agent apart from its bounded rationality. This conclusion comes from the same reason KL-Divergence constraints are introduced in the first place -- because we want to limit the rationality of agent alone. 

\begin{tcolorbox}
\textbf{Lesson 3: }Lagrange multiplier's method doesn't make a lot of senses (explanation needed) + not extensible, therefore we need a graphical model to explain this.
\end{tcolorbox}
\emph{\textbf{Note}: Balancing Soft-Q still got other problems, which we explored later.}  

\subsection{Deriving Optimal Policy for Balancing Soft-Q}
Let's see how to derive the optimal policy from the objective. This will be a common pattern when we derive other algorithms. Furthermore, this is based on \cite{levine2018reinforcement} message passing, however, considering only the base case. We would like to find the optimal opponent by maximising the following objective with respect to $\rho_{\phi}$ (We also define a normalizing term/message to be $N(\boldsymbol{s}, \boldsymbol{a}^i)$. The real definition will be clear afterwards)

\begin{equation*}
    \begin{aligned}
        \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i)}&\bigg[ \mathbb{E}_{q(\boldsymbol{a}^{-i} | \boldsymbol{s})} \bigg[ R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) - \frac{1}{\beta_{-i}}\log \frac{\rho_{\phi}(\boldsymbol{a}^i | \boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s})}  \bigg] - \frac{1}{\beta_{i}}\log \frac{\pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})} \bigg] \\
        &= \begin{aligned}[t]
            \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i)} \bigg[ \mathbb{E}_{q(\boldsymbol{a}^{-i} | \boldsymbol{s})} \bigg[ R&(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) - \frac{1}{\beta_{-i}}\log \rho_{\phi}(\boldsymbol{a}^i | \boldsymbol{s})  \\
            &+\frac{1}{\beta_{-i}} \log P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}) - N(\boldsymbol{s}, \boldsymbol{a}^{i}) + N(\boldsymbol{s}, \boldsymbol{a}^{i}) \bigg] \\
            &+\frac{1}{\beta_{i}}\log \frac{\pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})} \bigg]
        \end{aligned} \\
        &= \begin{aligned}[t]
            \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})}[N(\boldsymbol{s}, \boldsymbol{a}^i)] - \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i)} \bigg[ \mathbb{E}_{q(\boldsymbol{a}^{-i} | \boldsymbol{s})} \bigg[ -R&(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) + \frac{1}{\beta_{-i}}\log \rho_{\phi}(\boldsymbol{a}^i | \boldsymbol{s})  \\
            &-\frac{1}{\beta_{-i}} \log P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s})+ N(\boldsymbol{s}, \boldsymbol{a}^{i}) \bigg] \\
            &+\frac{1}{\beta_{i}}\log \frac{\pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})}\bigg]
        \end{aligned} \\
        &= \begin{aligned}[t]
            \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})}[N(\boldsymbol{s}, \boldsymbol{a}^i)] - \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i)} \bigg[ \mathbb{E}_{q(\boldsymbol{a}^{-i} | \boldsymbol{s})} \bigg[ -\beta_{-i} R&(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) + \log \rho_{\phi}(\boldsymbol{a}^i | \boldsymbol{s})  \\
            &-\log P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s})+ N(\boldsymbol{s}, \boldsymbol{a}^{i}) \bigg] \\
            &+\frac{\beta_{-i}}{\beta_{i}}\log \frac{\pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})}\bigg]
        \end{aligned}  \\
        &= \begin{aligned}[t]
            \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})}[N(\boldsymbol{s}, \boldsymbol{a}^i)] - \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i)} \bigg[ \mathbb{E}_{q(\boldsymbol{a}^{-i} | \boldsymbol{s})} \bigg[ &\log \rho_{\phi}(\boldsymbol{a}^i | \boldsymbol{s}) \\
            &-\bigg[ \beta_{-i} R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})   +\log P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}) - 
            N(\boldsymbol{s}, \boldsymbol{a}^{i}) \bigg] \bigg] \\
            &+\frac{\beta_{-i}}{\beta_{i}}\log \frac{\pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})}\bigg]
        \end{aligned} \\
        &= \begin{aligned}[t]
            \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})}[ N(\boldsymbol{s}, \boldsymbol{a}^i)] - \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i)} \Bigg[ &D_{KL}\Bigg[ \rho_{\phi}(\boldsymbol{a}^i | \boldsymbol{s}) \Bigg|\Bigg| \frac{\exp(\beta_{-i} R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}))P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s})}{\exp N(\boldsymbol{s}, \boldsymbol{a}^{i})} \Bigg] \Bigg] \\  &+\frac{\beta_{-i}}{\beta_{i}}\log \frac{\pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})}\bigg]
        \end{aligned} 
    \end{aligned}
\end{equation*}
\textbf{Note: } We absolve the constant $\beta_{-i}$ inside the normalizing factor. The optimal opponent is then equal to (There is a problem with $\boldsymbol{a}^i$. However, we care about using its policy to find the final value.)
\begin{equation}
    \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}) = \frac{\exp(\beta_{-i} R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}))P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s})}{\exp N(\boldsymbol{s}, \boldsymbol{a}^{i})}
\end{equation}
This also leads to a normalizing factor of 
\begin{equation}
    N(\boldsymbol{s}, \boldsymbol{a}^{i}) = \frac{1}{\beta_{-i} }\log \int_{\mathcal{A}^{-i}} \exp(R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}))P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s})
\end{equation}
We can interpret this as the game's Q-value function based on prior knowledge of opponent \cite{grau2018balancing} in the view of agent's. After getting the optimal opponent that is solely based on $R(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{a}^{-i})$, now we can find the agent optimal policy by substitute this optimal opponent back, and optimize the agent's policy instead.

\begin{equation*}
    \begin{aligned}
        &\mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})}\bigg[ R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})  - \frac{1}{\beta_{i}}\log \frac{\pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})}  - \frac{1}{\beta_{-i}}\log \frac{\exp(\beta_{-i} R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}))\cancel{P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s})}}{\exp  N(\boldsymbol{s}, \boldsymbol{a}^{i}) \cancel{P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s})} } \bigg] \\
        &= \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})}\bigg[ \cancel{R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})}  - \frac{1}{\beta_{i}}\log \frac{\pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})}  - \cancel{R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}))} + \underbrace{ \frac{1}{\beta_{-i}}\log \exp N(\boldsymbol{s}, \boldsymbol{a}^{i})}_{R(\boldsymbol{s}, \boldsymbol{a}^i)} \bigg]\\
        &= \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})}\bigg[ \beta_i R(\boldsymbol{s}, \boldsymbol{a}^i) - \log \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}) +  P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}) + N(\boldsymbol{s}) - N(\boldsymbol{s}) \bigg] \\
        &= \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})}[N(\boldsymbol{s})] - 
        \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})}\bigg[ -\beta_i R(\boldsymbol{s}, \boldsymbol{a}^i) + \log \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}) -  P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}) + N(\boldsymbol{s}) \bigg] \\
        &= \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})}[N(\boldsymbol{s})] - 
        \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})}\bigg[\log \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}) - \bigg[\beta_i R(\boldsymbol{s}, \boldsymbol{a}^i) +P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}) -  N(\boldsymbol{s}) \bigg] \bigg] \\
        &= \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^i \boldsymbol{a}^{-i})}[N(\boldsymbol{s})] - 
        \mathbb{E}_{q(\boldsymbol{s},  \boldsymbol{a}^{-i})}\bigg[D_{KL}\bigg[ \log \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}) \bigg|\bigg| \frac{\exp\left( \beta_i R(\boldsymbol{s}, \boldsymbol{a}^i)\right) P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})}{N(\boldsymbol{s})}  \bigg]\bigg]
    \end{aligned} 
\end{equation*}
In order to max the objective we have to set $\pi_{\theta}$ to be equal to 

\begin{equation*}
    \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}) = \frac{\exp\left( \beta_i R(\boldsymbol{s}, \boldsymbol{a}^i)\right) P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})}{N(\boldsymbol{s})}
\end{equation*}
This corresponds directly to \cite{grau2018balancing} optimal policy. However, if we can see, the term $R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})$ got cancelled due to the Lagrange multiplier on $\log \frac{\rho_{\phi}(\boldsymbol{a}^i | \boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s})}$. With this observation and our derivation of Balancing Soft-Q, we can conclude that: 

\begin{tcolorbox}
\textbf{Lesson 4: }\label{lesson:4} We need to have an \textbf{independent inference method for calculating opponent's model}. \cite{grau2018balancing} works can be seen as non-decentralized version of ROMMEO (since we directly use \textit{opponent's optimal policy against agent's prior or simply ignore it} the agent's policy should be $\pi(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i})$)
\end{tcolorbox}

\begin{tcolorbox}
\textbf{Lesson 5: } There are 2 steps we have to make: first finding the extreme case of the value function, then we can find a policy.  
\end{tcolorbox}

\section{Unified View}
\subsection{ROMMEO with Balancing Soft-Q}
Since we are going to split the graphical model, we have to find an optimal $\pi(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i})$, with the assumption of playing against an unknown opponent (prior of opponent). Furthermore, given the opponent's policy, the most optimal thing for an agent to do is just to maximizes its reward.

\begin{figure}[ht]
    \begin{minipage}[t]{0.5\linewidth}
    \centering
    \begin{tikzpicture}[latent/.append style={minimum size=1.0cm}]
        \node[obs] (o) {$\mathcal{O}^i$};
        \node[obs, below=of o] (a) {$\boldsymbol{a}^i$};
        \node[latent, below=of a] (s) {$\boldsymbol{s}$};
        \node[latent, right=of a] (a1) {$\boldsymbol{a}^{-i}$};
        % \node[latent, right=2.3cm of o] (o1) {$\mathcal{O}^{-i}$};
        
        % above right=1.3cm of a
        
        \edge {s} {a, a1}
        \edge {a} {o}
        % \edge {o1} {o, a1}
        % \edge {a1} {a}
        \edge {a1} {o}
        \path[->]  (s)  edge   [bend left=30] node {} (o);
    \end{tikzpicture}
    \end{minipage}%
    \begin{minipage}[t]{0.5\linewidth}
    \captionof{figure}{Graphical model of splitted graphical model for agent's policy. } 
    % \label{fig:}
    \end{minipage}
\end{figure}
Start with finding the joint probability of the graphical model.
\begin{equation}
    P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}) P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}) P(\boldsymbol{s}) P(\mathcal{O}^i = 1 | \boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})
\end{equation}
We can see that the variational distribution can be defined as: (assuming that the )
\begin{equation}
    P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}) \pi_{\theta}(\boldsymbol{a}^{i} | \boldsymbol{s}, \boldsymbol{a}^{-i}) P(\boldsymbol{s})
\end{equation}
Then the approximate variational inference can be done by minimising the KL-Divergence, hence, we have the following objective 

\begin{equation}
    \begin{aligned}
        &\begin{aligned}
            \mathbb{E}_{q(\boldsymbol{a}^{i}, \boldsymbol{a}^{-i}, \boldsymbol{s})}\bigg[&\cancel{\log P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s})} + \log \pi_{\theta}(\boldsymbol{a}^{i} | \boldsymbol{s}, \boldsymbol{a}^{-i}) + \log \cancel{P(\boldsymbol{s})} - \log P_{\text{prior}}(\boldsymbol{a}^{i} | \boldsymbol{s}, \boldsymbol{a}^{-i}) \\
            &- \cancel{\log P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s})} - \cancel{\log P(\boldsymbol{s})} -\log P(\mathcal{O}^i = 1 | \boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \bigg]
        \end{aligned} \\ 
        &= \mathbb{E}_{q(\boldsymbol{a}^{i}, \boldsymbol{a}^{-i}, \boldsymbol{s})}\bigg[ -\beta_i R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) + \log \frac{\pi_{\theta}(\boldsymbol{a}^{i} | \boldsymbol{s},\boldsymbol{a}^{-i})}{P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \boldsymbol{a}^{-i})} \bigg] \\ 
    \end{aligned}
\end{equation}
This is equal to maximizing the following objective 
\begin{equation}
    \mathbb{E}_{q(\boldsymbol{a}^{i}, \boldsymbol{a}^{-i}, \boldsymbol{s})}\bigg[\beta_i R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) - \log \frac{\pi_{\theta}(\boldsymbol{a}^{i} | \boldsymbol{s},\boldsymbol{a}^{-i})}{P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \boldsymbol{a}^{-i})} \bigg]
\end{equation}
We can find the solution in a closed form by having 
\begin{equation}
    \begin{aligned}
        &\mathbb{E}_{q(\boldsymbol{a}^{i}, \boldsymbol{a}^{-i}, \boldsymbol{s})}\bigg[\beta_i R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) - \log \frac{\pi_{\theta}(\boldsymbol{a}^{i} | \boldsymbol{s},\boldsymbol{a}^{-i})}{P_{\text{prior}}(\boldsymbol{a}^{i} | \boldsymbol{s}, \boldsymbol{a}^{-i})} + N^{i}(\boldsymbol{s}, \boldsymbol{a}^{-i}) - N^{i}(\boldsymbol{s}, \boldsymbol{a}^{-i}) \bigg] \\
        & = \begin{aligned}[t]
            \mathbb{E}_{q(\boldsymbol{a}^{i}, \boldsymbol{a}^{-i}, \boldsymbol{s})}&\bigg[ N^{i}(\boldsymbol{s}, \boldsymbol{a}^{-i}) \bigg] - \mathbb{E}_{q(\boldsymbol{a}^{i}, \boldsymbol{a}^{-i}, \boldsymbol{s})}\bigg[ \log \pi_{\theta}(\boldsymbol{a}^{i} | \boldsymbol{s},\boldsymbol{a}^{-i}) \\
            &- \bigg[ \log \exp\Big(\beta_i R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})\Big) - \log \exp N^{i}(\boldsymbol{s}, \boldsymbol{a}^{-i}) + \log P_{\text{prior}}(\boldsymbol{a}^{i} | \boldsymbol{s}, \boldsymbol{a}^{-i})  \bigg] \bigg]
        \end{aligned} \\
        &= \mathbb{E}_{q(\boldsymbol{a}^{i}, \boldsymbol{a}^{-i}, \boldsymbol{s})}\left[ N^{i}(\boldsymbol{s}, \boldsymbol{a}^{-i}) \right] - \mathbb{E}_{q(\boldsymbol{a}^{-i}, \boldsymbol{s})}\left[ D_{KL}\left[ \pi_{\theta}(\boldsymbol{a}^{i} | \boldsymbol{s},\boldsymbol{a}^{-i}) \Bigg|\Bigg| \frac{\exp(\beta_i R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})) P_{\text{prior}}(\boldsymbol{a}^{i} | \boldsymbol{s}, \boldsymbol{a}^{-i})}{ \exp N^{i}(\boldsymbol{s}, \boldsymbol{a}^{-i})} \right]\right] 
    \end{aligned}
\end{equation}
We can see that, we can easily see that if want to maximizes the equation, we can set 
\begin{equation}
    \pi_{\theta}(\boldsymbol{a}^{i} | \boldsymbol{s},\boldsymbol{a}^{-i}) = \frac{\exp(\beta_i R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})) P_{\text{prior}}(\boldsymbol{a}^{i} | \boldsymbol{s}, \boldsymbol{a}^{-i})}{ \exp N^{i}(\boldsymbol{s}, \boldsymbol{a}^{-i})}
\end{equation}
Where $N^{i}(\boldsymbol{s}, \boldsymbol{a}^{-i})$ can be interpreted as the max expected return of the agent given the current state and opponent action based on agent's prior action.
\begin{equation}
    N^{i}(\boldsymbol{s}, \boldsymbol{a}^{-i}) = \log \int \exp(\beta_i R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})) P_{\text{prior}}(\boldsymbol{a}^{i} | \boldsymbol{s}, \boldsymbol{a}^{-i}) \ d\boldsymbol{a}^i
\end{equation}
Since we found a true $\pi_{\theta}(\boldsymbol{a}^{i} | \boldsymbol{s},\boldsymbol{a}^{-i})$ we know that the optimizing objective is left with only $N^{i}(\boldsymbol{s}, \boldsymbol{a}^{-i})$, which we can use in finding optimal opponent's policy. But first, let's define the graphical model for opponent's policy.

\begin{figure}[ht]
    \begin{minipage}[t]{0.5\linewidth}
    \centering
    \begin{tikzpicture}[latent/.append style={minimum size=1.0cm}]
        \node[obs] (o) {$\mathcal{O}^{-i}$};
        \node[latent, below=of o] (a1) {$\boldsymbol{a}^{-i}$};
        \node[latent, below=of a1] (s) {$\boldsymbol{s}$};
        % \node[latent, right=2.3cm of o] (o1) {$\mathcal{O}^{-i}$};
        
        % above right=1.3cm of a
        
        \edge {s} {a1}
        % \edge {a} {o}
        % \edge {o1} {o, a1}
        % \edge {a1} {a}
        \edge {a1} {o}
        \path[->]  (s)  edge   [bend left=30] node {} (o);
    \end{tikzpicture}
    \end{minipage}%
    \begin{minipage}[t]{0.5\linewidth}
    \captionof{figure}{Graphical model of splitted graphical model for opponent's policy. } 
    % \label{fig:}
    \end{minipage}
\end{figure}

Surprisingly, the opponent's policy optimality can be defined to be 
\begin{equation}
    P(\mathcal{O}^{-i} = 1 | \boldsymbol{s}, \boldsymbol{a}^{-i}) = \exp\left(\frac{\beta_{-i}}{\beta_{i}} N^{i}(\boldsymbol{s}, \boldsymbol{a}^{-i})\right)
\end{equation}
This can be justified in 2 ways 
\begin{itemize}
    \item This is based on \cite{grau2018balancing}, where it states that inaction selection for the opponent model  "depends on the action of our actor, which is unknown a priori". Strictly speaking, this is also based on the $\arg\max\text{ext}$ of the value function (what we have left from maximizing agent's return from a previous graphical model is the $\max$ of the value function). 
    \item Having $\beta_{-i}$ implies that the opponent model will have the characteristic, which is based on the soft-maximum of game's return given the prior over agent's behaviour (since we also "normalized by" $\frac{1}{\beta_i}$). 
    The Normalizing term $\frac{1}{\beta_{\pm i}}$ becomes important with the case of PR2's\cite{wen2019probabilistic} opponent model $\rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \boldsymbol{a})$ in zero sum game, since, without this normalizing term $N(\boldsymbol{s}, \boldsymbol{a}^{i})$ will be based on soft-maximum of oppoent's return, which isn't what we intended to be (since the most optimal for an agent is to reduce the return or maximizes game's reward).
\end{itemize}
With this, we can define the joint distribution to be 
\begin{equation}
    P(\boldsymbol{s}) P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}) P(\mathcal{O}^{-i} = 1 | \boldsymbol{s}, \boldsymbol{a}^{-i})
\end{equation}
The variational distribution is 
\begin{equation}
    P(\boldsymbol{s}) \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s})
\end{equation}
% Then the approximate variational inference becomes 

Finally, we can set 
\begin{equation}
    \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}) = \frac{\exp\left(\beta_{-i} R(\boldsymbol{s}, \boldsymbol{a}^{-i})) P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}\right)}{\exp N(\boldsymbol{s})}
\end{equation}
where 
\begin{equation}
    N(\boldsymbol{s}) = \log \int_{\mathcal{A}^{-i}} \exp(\beta_{-i} R(\boldsymbol{s}, \boldsymbol{a}^{-i})) P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}) \quad \text{ and } \quad  R(\boldsymbol{s}, \boldsymbol{a}^{-i}) = \frac{1}{\beta_{i}}N(\boldsymbol{s}, \boldsymbol{a}^{-i})
\end{equation}
We can show that ROMMEO with the following objective 
\begin{equation}
    \mathbb{E}\Bigg[ R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) - \frac{1}{\beta_i} \log \frac{\pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i})}{P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i})} - \frac{1}{\beta_{-i}} \log \frac{\rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s})}  \Bigg]
\end{equation}
Yields almost the same result. Thus, we have ROMMEO that works for both zero-sum game and team game, represented in a separates graphical model. 
% \subsection{ROMMEO with Balanced-Q}

% \begin{equation}
%     \mathbb{E}_{q(\boldsymbol{s}, \boldsymbol{a}^{-i}, \boldsymbol{a}^{-i})}
% \end{equation}


















% \section{Unified View}
% \subsection{Single-Step Balancing Soft-Q \cite{grau2018balancing} with ROMMEO \cite{tian2019regularized}}
% \subsection{Single-Step PR2 \cite{wen2019probabilistic}}
% \subsection{Single-Step GR2 \cite{wen2019multi}}

% \section{Additional Features}
% \subsection{Belief Over Agent's Characteristic}
% \subsection{Extension to General Sum Game}


