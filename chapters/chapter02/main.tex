\label{chapter:chap2}
\begin{adjustwidth}{1cm}{}
    % In this chapter, we will provide and reviews tools that will be useful for deriving probabilistic multi-agent reinforcement learning algorithms with a formal setting of the problem that the algorithms are trying to solve. We will start by going through the probabilistic machine learning, which provides the tools for mitigate an inevitable intractable problem (will discuss in more details). After getting all the fundamental tools, we will move to single agent reinforcement learning section, which, in the first part, will provide classical results, and theorem. In the later section, we will investigate control-as-inference framework and its extensions. Finally, we will finish with an introduction to multi-agent reinforcement learning with Multi-agent with probabilistic inference framework (MAPI) - our main focus. Our main focus for this chapter is to provide readers backgrounds of the current sub-field, and how they are developed, which will help them understand the motivation for our result more clearly.
    The main aim of this chapter is to make the readers familiar with the background required to understand the development of the algorithms proposed in this thesis. We try to keep this chapter as small as possible. As noted before, any specific algorithms that will only be discussed in a particular chapter will not be described here; this will also help to find related information more accessible. At the end of this chapter, in section \ref{sec:chap2-other-algos}, we will list the algorithms that will be introduced, given the corresponding chapters.
\end{adjustwidth}


\section{Probabilistic Method in Machine Learning}
\input{chapters/chapter02/contents/01_probabilistic_methods}

\section{Single Agent Reinforcement Learning}
\input{chapters/chapter02/contents/02_single_rl}

\section{Deep Reinforcement Learning}
\input{chapters/chapter02/contents/03_deep_rl}

\section{Control as Inference Framework}
\input{chapters/chapter02/contents/04_probrl}

\section{Introduction to Multi-Agent Problems}
\input{chapters/chapter02/contents/05_classic_marl}

\section{Deep Multi-Agent Reinforcement Learning}
\input{chapters/chapter02/contents/06_deep_marl}

\section{Other Related Algorithms}
\input{chapters/chapter02/contents/07_other_algo}

% \section{Extensions to control as inference frameworks}
% After we have introduced control and inference framework, we will now present current progress of reinforcement learning made based on this framework. We will start with hierarchical agent, which is simplest extension to the framework. After this, we will look on how can be represents the unknown environment with belief over the state, which would requires us to formulate the problem into partially observable Markov decision process (POMDP). Finally, we will take a slight turn and examine current work on information theoretic inspired single agent reinforcement learning algorithms.

% \section{Multi-agent reinforcement learning}
% In this section, we will start formulating Multi-agent reinforcement learning problems, by first introducing a formulation of decentralized partially observable Markov decision process (DEC-POMDP), introducing some game theoretic concepts, with more traditional algorithms including Nash-Q \cite{hu2003nash}, MADDPG \cite{lowe2017multi}, COMA \cite{foerster2018counterfactual}, where some of them will be our baseline for our algorithms.

% \section{Multi-agent with probabilistic inference framework}
% Finally, we will present current algorithms developed by re-interpretation multi-agent reinforcement learning problem as control-as-inference framework. We will present 4 algorithms based on this: PR2 \cite{wen2019probabilistic}, ROMMEO \cite{tian2019regularized}, GR2 \cite{wen2019multi}, and Balancing-Q \cite{grau2018balancing}. All of them will be examined in full details because they lays a foundations to main result of this thesis. We will end this chapter by shinning light into incompatibility between these algorithms, which will solve in the later chapters. 