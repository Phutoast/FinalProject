\label{chapter:chap2}
\begin{adjustwidth}{1cm}{}
    In this chapter, we will provide and reviews tools that will be useful for deriving probabilistic multi-agent reinforcement learning algorithms with a formal setting of the problem that the algorithms are trying to solve. We will start by going through the probabilistic machine learning, which provides the tools for mitigate an inevitable intractable problem (will discuss in more details). After getting all the fundamental tools, we will move to single agent reinforcement learning section, which, in the first part, will provide classical results, and theorem. In the later section, we will investigate control-as-inference framework and its extensions. Finally, we will finish with an introduction to multi-agent reinforcement learning with Multi-agent with probabilistic inference framework (MAPI) - our main focus. Our main focus for this chapter is to provide readers backgrounds of the current sub-field, and how they are developed, which will help them understand the motivation for our result more clearly.
\end{adjustwidth}

\section{Probabilistic Machine learning}
\label{chap2:prob-ml}

Most of the focus in this section will be to provide techniques that provides a walk-around solution to Bayes' rule, suppose we want to infer a parameter $\theta$ given training data $X$ and training label $Y$ in supervised learning (however, this can also be used in unsupervised learning, notably Gaussian Mixture Model): 

\begin{equation}
    \label{eqn:bayes-theorem}
    P(\theta | X, Y) = \frac{P(Y | X, \theta) P(\theta)}{P(Y | X)} = \frac{P(Y | X, \theta) P(\theta)}{\int P(Y | X, \theta) P(\theta ) \dby \theta}
\end{equation}
Furthermore, given the input data point $x^*$ that we would like to make prediction $y^*$ of, we can do marginalization of all parameters i.e
\begin{equation}
    \label{eqn:bayes-pred}
    \int P(y^* | x^*, X, Y, \theta) \dby\theta = \int P(y^* | x^*, \theta) P(\theta | X, Y) \dby\theta
\end{equation}
The biggest problem in Bayesian inference is when the integral is intractable. Note that in equation \ref{eqn:bayes-pred}, we can approximate it via Monte-Carlo sample (assuming we can sample $P(\theta | X, Y)$). However, the more important, and harder to solve is the denominator in equation \ref{eqn:bayes-theorem}. In this thesis, we will focus on variational inference techniques, however, there are other techniques for example, conjugate prior.


\Phu{Do i have to cite Learning in Graphical Models ?}
\subsection{Variational inference techniques}
\label{sec:vi-technique}
We would like to approximate the posterior $P(\theta | X, Y)$ with parametric distribution $q_{\phi} (\theta)$. This can be done by maximizing negative KL-divergence between them defined as:
\begin{equation}
    \kl\left( q_{\phi} (\theta) \Big\| P(\theta | X, Y) \right) = \int q_{\phi} (\theta) \log\left( \frac{q_{\phi} (\theta)}{P(\theta | X, Y)} \right) \dby \theta
\end{equation}
Note that given 2 difference distributions $q$ and $p$, $\kl(q \| p) \ne \kl(p \| q)$. \textbf{Minimizing} the KL-divergence is equivalent to \textbf{maximized} the following value, which we will called it Evidence lower bound (ELBO)
\begin{equation}
    \argmax{\theta} \mathbb{E}_{q_{\phi} (\theta)} \left[ P(Y | X, \theta) \right] - \kl \left(q_{\phi} (\theta) \Big\| P(\theta) \right) = \int q_{\phi}(\theta)\log\left(\frac{P(Y, \theta | X)}{q_{\phi}(\theta)}\right) \dby \theta
\end{equation}
We can calulate this without relying on $P(Y | X)$. The full derivation is in appendix \ref{appendix-1:elbo-kl} for completeness. Now, we will step back and define the log-likelihood, note that we can also derive ELBO from this quantity using Jensen's inequality, see appendix \ref{appendix-1:elbo-jensen}
\begin{equation}
    l(X, Y) = \log P(Y| X) = \log \left( \int P(Y,  \theta | X) \dby \theta \right)
\end{equation}
% We will treat $\theta$ as hidden variable, and then finding how can we increase the likelihood of this, which will provides the reason behind the maximizing ELBO. 
We can show the gap between this log-likelihood that we want to optimize and ELBO 
\begin{equation}
    l(X, Y) - \underbrace{\int q_{\phi}(\theta) \log\left(\frac{P(Y, \theta | X)}{q_{\phi}(\theta)}\right) \dby \theta}_{\text{ELBO}} = \int q_{\phi} (\theta) \log\left( \frac{q_{\phi} (\theta)}{P(\theta | X, Y)} \right) \dby \theta
\end{equation}
The derivation will be in appendix \ref{appendix-1:elbo-gap}. Interestingly, it is the same as the objective that we want to optimize earlier, which is KL-divergence between variational distribution and true posterior. We would like to note that minimizing this is possible, since it is shown earlier to be the same as maximizing ELBO but it is intractable to calculate its value. Finally, KL-divergence is always non-negative (this is known as Gibbs' inequality), therefore ELBO will stay lower bound, and optimizing ELBO will also optimize whole $l(X, Y)$

\subsection{EM Algorithm}
Now after we have explored the use of variational inference technique in supervised learning, we will now turn to unsupervised learning, and the use of EM algorithm. We will follows the example provided in \cite{fellows2019virel}. Suppose, we have a $N$ data points $X = \{x^{(n)}\}^N_{n=1}$ that we know that it is generated by hidden variable $z$. We would like to learn the "process" that generates such a data point, i.e the variable $\theta$, such that $P_{\theta}(X | z)$. It is clear that we would like to find $\theta$ that maximizes the log-likelihood of the observed data point
\begin{equation}
    l_{\text{unsup}}(\theta) = \log \left( P_{\theta}(X) \right) = \log\bracka {\int P_{\theta}(X, z) \dby z}
\end{equation}
With this, we can also introduce the variational distribution over the hidden variable (since we don't know what it is) $q_{\phi}(z)$, with similar pattern, we can decompose the log-likelihood into:
\begin{equation}
    \begin{aligned}
        l_{\text{unsup}}(\theta) &= \int q_{\phi}(z) \log \bracka{\frac{P(X, z)}{P(z | X)}} \dby z \\
        &= \int q_{\phi}(z) \log \bracka{\frac{P_{\theta}(X, z)}{q_{\phi}(z)}} \dby z + \int q_{\phi}(z) \log \bracka{\frac{q_{\phi}(z)}{P_{\theta}(z | X)}} \dby z
    \end{aligned}
\end{equation}
This can be arrived by similar method in section \ref{sec:vi-technique}, where ELBO is 
\begin{equation}
    \begin{aligned}
        \mathcal{L}(\theta, \phi) &= \int q_{\phi}(z) \log \bracka{\frac{P_{\theta}(X, z)}{q_{\phi}(z)}} \dby z \\
        &= \mathbb{E}_{q_{\phi}(z)} \brackb{\log P_{\theta}(X | z)} - \kl\bracka{q_{\phi}(z) \Big\| P(z)}
    \end{aligned}
\end{equation}
We will now describe each version of EM algorithms.
\paragraph{Exact EM algorithm} at step $k$, we first make the gap between ELBO and log-likelihood tight by setting 
\begin{equation}
    q_{\phi^{(k+1)}}(z) \leftarrow P_{\theta^{(k)}}(z | X)
\end{equation}
We call this Expectation-Step or E-step, then we can optimizing $\theta$ by optimizing the ELBO as:
\begin{equation}
    \theta^{(k+1)} \leftarrow \argmax{\theta^{(k)}}  \int P_{\theta^{(k)}}(z | X) \log \bracka{\frac{P_{\theta^{(k)}}(X, z)}{P_{\theta^{(k)}}(z | X)}} \dby z 
\end{equation}
We can this Maximization-Step or M-Step.

\paragraph{Variational EM} However, on E-step, we might not be able to update the variational distribution, we, therefore, resorted to minimizing KL-divergence between the true posterior of $z$ and the variational distribution: 
\begin{equation*}
    \argmax{\phi} \kl\left( q_{\phi} (z) \Big\| P_{\theta}(z | X) \right) = \argmax{\phi} \mathcal{L}(\theta, \phi)
\end{equation*}
which is the same as optimizing the ELBO with respect to $\phi$. Thus, we can perform variational EM as follows (at step $k$):
\begin{equation}
    \begin{aligned}
        &\text{E-Step: } \quad \phi^{(k+1)} \leftarrow \argmax{\phi^{(k)}}\mathcal{L}(\theta^{(k)}, \phi^{(k)}) \\
        &\text{M-Step: } \quad \theta^{(k+1)} \leftarrow \argmax{\theta^{(k)}}\mathcal{L}(\theta^{(k)}, \phi^{(k+1)})
    \end{aligned}
\end{equation}

\grammar{This is used in variational auto-encoder \cite{kingma2013auto} where by we parameterized encoder as $q_{\phi}(z | X) = \mathcal{N}(z ; \mu_{\phi}(X), \Sigma_{\phi}(X))$, where $\mu_{\phi}$ and $\Sigma_{\phi}$ are neural network. For, the decoder as $P_{\theta}(X | z) = \mathcal{N}(X ; \mu_{\theta}(z), \Sigma_{\theta}(z))$. However, for decoder we usually ignore the covariance matrix, and for encoder, we tends to assume independences between each latent variables (diagonal covariance matrix) for tractable computation. Since we have both parameters available, we can train both neural networks at the same time by optimizing the ELBO.}
\begin{equation}
    \mathbb{E}_{q_{\phi}(z | X)} \brackb{\log P_{\theta}(X | z)} - \kl\bracka{q_{\phi}(z | X) \Big\| P(z)}
\end{equation}
\grammar{The authors used re-parametrisation trick (pathwise derivative \cite{gal2016uncertainty}) in order to optimize the first quantity in ELBO. Generally, suppose we want to find the gradient of $\mathbb{E}_{a \sim p_{\theta}}[f(a)]$. This can be done by finding differentiable function $g(\theta, \varepsilon)$, where $\varepsilon \sim \mathcal{X}$ and $\mathcal{X}$ is arbitrary distribution that we can sample, such that $\mathbb{E}_{\varepsilon \sim \mathcal{X}}\brackb{f(g(\theta, \varepsilon))} = \mathbb{E}_{a \sim p_{\theta}}[f(a)]$. For example, for normal distribution $\mathcal{N}(x; \mu, \sigma)$, we have $g(\mu, \sigma, \varepsilon) = \mu +\sigma\varepsilon$ where $\varepsilon \sim \mathcal{N}(\varepsilon;0, 1)$. The gradient, thus, can easily be computed by chain rule.}

\subsection{SVGD \cite{liu2016stein}}
SVDG is used in many of the algorithm that we are presenting (for example soft-q-learning \cite{haarnoja2017reinforcement} and PR2 \cite{wen2019probabilistic}). It is a "general purpose variational inference algorithm", which can also be used to "sample" the data point from complex distribution (optimized variational distribution). This is extremely useful since we want to construct and use neural network based variational distribution for our agents. \grammar{The algorithm itself is simple, yet, having very interesting mathematical formulation behind it. We suggest interested reader to check out the paper. We will described the general form of the algorithm, while the details will be left for each of its usage.} Suppose that we want to estimate the distribution $P$ (which can be unnormalized) by a set of $n$ particles $\{\xi^{(0)}_i\}^n_{i=1}$. Then we can update the set of such particles such that the final set of $n$ particles $\{\xi^{(T)}_i\}^n_{i=1}$ represent sampled point from the distribution $P$. The algorithm does the following update rule (for iteration at step $t$): 
\begin{equation}
    x^{(t+1)}_i \leftarrow x^{(t)}_i + \alpha \bracka{\frac{1}{n}\sum^n_{j=1} \brackb{k(x^{(t)}_j, x^{(t)}_i) \nabla_{x^{(t)_j}} \log P(x^{(t)}_j) + \nabla_{x^{(t)_j}} k (x^{(t)}_j, x^{(t)}_i) }}
\end{equation}
where $\alpha$ is the updating step. \factcheck{The author has shown that this update rule follows the functional gradient of KL-divergence between the variational distribution ($q_{[T]}$ which is the density of $z = T(\xi)$) and the true distribution $P$ (Theorem 3.3). See the original paper for more details.}

\section{Single agent Reinforcement Learning}

\Phu{citing the courses and book ?}

We can visualize single agent reinforcement learning problem as single player game, in which we are given the controller and a game. We will try to gain maximum rewards by learning from experiences. We will introduce the formulation of Markov decision process (MDP), and how to solve them either model-based or model-free. After understanding the settings and some plausible solutions, we will interpret reward maximization as inference and use the tool from previous section (\ref{chap2:prob-ml}) to solve an inference problem, which, surprisingly, have noticeable connection to more traditional machine learning that we presented earlier in the section. 

\subsection{Classical Reinforcement Learning}
We will start with the definition of Markov decision process (MDP), which formulates the games as the followings.
\begin{definition}
    Markov decision process (MDP) is a tuple : $\langle S, A, R, \mathcal{T}, p_0, \gamma \rangle$ where 
    \begin{itemize}
        \item State space: $S = \{s_0, s_1, \cdots , s_{|S|}\}$. Usually we have $s_i \in \mathbb{R}^{d_s}$
        \item Action space: $A = \{a_0, a_1, \cdots a_{|A|}\}$
        \item Reward function: $\mathcal{R}: S \times A \rightarrow \mathbb{R}$
        \item Transition function: $\mathcal{T}: S \times A \times S \rightarrow [0, 1]$, where $s^{(t+1)} \sim \mathcal{T}(s^{(t+1)} | s^{(t)}, a^{(t)})$ 
        \item Initial state distribution: $p_0 : S \rightarrow [0, 1] $, where $s^{(0)} \sim p_0$
        \item Discounted factor: $\gamma \in (0, 1]$
    \end{itemize}
    In almost all the cases, we include player's policy $\pi: S \rightarrow \Delta(A)$ where $\Delta(A)$ is probability simplex over $A$, where $a^{(t)} \sim \pi(a^{(t)} | s^{(t)})$ denote player choose action $a^{(t)}$ at state $s^{(t)}$
\end{definition}
\noindent
We denote the value function of the player's policy $V^\pi(s)$ as 
\begin{equation}
    V^\pi(s) = \mathbb{E}_{a^{(t)} \sim \pi, s^{(t+1)}\sim \mathcal{T}}\brackb{\sum^\infty_{t=0} \gamma^t r(s_t, a_t) \Bigg| s^{(0)} = s}
\end{equation}
And the action value function is defined as 
\begin{equation}
    Q^\pi(s, a) = \mathbb{E}_{a^{(t)} \sim \pi, s^{(t+1)}\sim \mathcal{T}}\brackb{\sum^\infty_{t=0} \gamma^t r(s_t, a_t) \Bigg| s^{(0)} = s, a^{(0)} = a}
\end{equation}
We can establish the connection between both functions as
\begin{equation}
    \label{eqn:Q-and-V}
    Q^\pi(s, a) = r(s, a) + \gamma \mathbb{E}_{s' \sim \mathcal{T}(s' | s, a)}\brackb{V^\pi(s')} \quad \quad \quad V^\pi(s) = \mathbb{E}_{a \sim \pi(a | s)} \brackb{Q^\pi(s, a)}
\end{equation}
Furthermore, the advantage function is defined as 
\begin{equation}
    A^{\pi}(s, a) = Q^\pi(s, a) - V^\pi(s)
\end{equation}
this function estimate the "advantage" for the agent if it chooses action $a$ at state $s$.  The goal of the agent is to find an optimal policy $\pi^* \in \argmax{\pi} V^{\pi}(s)$ for any states $s$ (there is such an value for all states). We dentoe the optimal value function and optimal action value function as $V^*(s)$ and $Q^*(s, a)$ as $V^{\pi^*}(s)$ and $Q^{\pi^*}(s, a)$, respectively. Now, we want to establish connection between these 2 functions, which is:
\begin{equation}
    Q^*(s, a) = r(s, a) + \gamma \mathbb{E}_{s' \sim \mathcal{T}(s' | s, a)}\brackb{V^*(s')} \quad \quad \quad V^*(s) =\max_{a\in A} Q^*(s, a)
\end{equation}

\subsubsection{Policy Iteration}
We will introduce the first algorithm that can solve the problem, which is \textit{policy iteration}. The strategy is simple with simple observation: the agent will always doing better or equal if we set it's behavior to be (following the optimal value function above)
\begin{equation}
    \label{eqn:greedy-Q}
    \pi'(a | s) = \begin{cases}
        1 &\text{ if } a = \argmax{a \in A} Q^\pi(s, a) \\
        0 &\text{ otherwise }
    \end{cases}
\end{equation}
We call this \textit{policy improvement} step. One can show that $\forall s \in S: V^\pi(s) \le V^{\pi'}(s)$. Furthermore, we can show that there exists a value function that in every state it has higher values if we keep improving the policy. The proof, for completeness, will be presented in appendix \ref{appendix-2:policy-im}. What we have left is to find an \textit{policy evaluation} algorithm that gives us value function $V^{\pi}(s)$ for any policy $\pi$. We can expand the equality in equation \ref{eqn:Q-and-V} for the value function as follows:
\begin{equation}
    V^\pi(s) = \mathbb{E}_{a \sim \pi(a | s)} \brackb{r(s, a) + \gamma \mathbb{E}_{s' \sim \contractop(s' | s, a)}\brackb{V^\pi(s')}} 
\end{equation}
We call this equation expected Bellman equation along with the following expected Bellman operator $\contractop^{\pi} : \mathbb{R}^{|S|} \rightarrow \mathbb{R}^{|S|}$ defined as:
\begin{equation}
    \label{eqn:exp-bellman-operator}
    \contractop^{\pi} V(s) = \mathbb{E}_{a \sim \pi(a | s)} \brackb{r(s, a) + \gamma \mathbb{E}_{s' \sim \contractop(s' | s, a)}\brackb{V(s')}} 
\end{equation}
Normally we have to find $V(s)$ such that $\contractop^\pi V(s) = V(s)$, which involves matrix inversion (see appendix \ref{appendix-2:metrics-rl} for more details). However, turn out this expected Bellman operator is an contraction mapping on $\infty$-norm i.e 
\begin{equation*}
    \| \contractop^\pi V_1(s) - \contractop^\pi V_2(s) \|_\infty \le \alpha \|V_1(s) - V_2(s)\|_\infty
\end{equation*}
For some $0 \le \alpha < 1$ (See appendix \ref{appendix-2:expect-bellman-contract} for proof). And by the following theorem (We will also present the proof of this theorem in appendix \ref{appendix-2:fixed-point}) 
\begin{theorem}{(Banach fixed-point theorem)}
    Given the complete (every Cauchy sequence converges to a point in that space) metric space $(X, d(\cdot, \cdot))$ and the contraction mapping $\contractop : X \rightarrow X$, the sequence $(\contractop^{(n)} x)^{\infty}_{n=1}$ converges to a fixed point $x^*$ where $\contractop x^* = x^*$, for all points $x \in X$. 
\end{theorem}
Repeatedly apply the Bellman operator will lead us to a solution of $V^{\pi}$.  In conclusion, Policy iteration is an algorithm that solves the MDP by involving the alternation between the following 2 steps
\begin{enumerate}
    \item \textbf{Policy Evaluation}: Starting with randomized value function and Repeatedly applying Bellman operator defined in equation \ref{eqn:exp-bellman-operator} until converge to get true value function $V^{\pi}(s)$.
    \item \textbf{Policy Improvement}: Update the policy by choosing the best action from action value function (we can calculate this by the equation \ref{eqn:Q-and-V}) following the equation \ref{eqn:greedy-Q}
\end{enumerate}
Since the value function always increases in every state, this algorithm is guaranteed to reach the optimal value function and policy. 

\subsubsection{Value Iteration}
Now, it is quite computational ineffective to evaluates the policy every times to update the policy. Can we just update the value function once and then uses the policy improvement to come-up with the next value function. We will define optimal value Bellman update operator as:
\begin{equation}
    \label{eqn:optimal-bellman-operator}
    \contractop^*_V V(s) = \max_{a \in A}\Big(r(s, a) + \gamma \mathbb{E}_{s' \sim \mathcal{T}(s' | s, a)}\brackb{V(s')} \Big)
\end{equation}
In this case, we update the the value function toward the action that yields highest value given only one update. It is clear that $V^*(s)$ is the fixed point of this operator. This is, indeed, a contraction mapping, thus repeatedly updating the value function by optimal value Bellman operator will give us optimal value function. The prove will be appendix \ref{appendix-2:optimal-val-bellman-contract}. Finally, given the optimal value function, one can calculate the optimal action value function, and optimal policy. 

\subsubsection{Q Iteration}
\label{sec:Q-iteration}

Now, instead of using value function as in value iteration, it is always desirable for us to estimate optimal action value function $Q^*(s, a)$, simply because the agent can greedily choose the action $a$ that maximizes this optimal action value function, see equation \ref{eqn:greedy-Q}. One can define the optimal action value Bellman operator as 
\begin{equation}
    \contractop^*_Q Q(s, a) = r(s, a) + \gamma \mathbb{E}_{s' \sim \mathcal{T}(s' | s, a)}\brackb{\max_{a\in A} Q(s', a)} 
\end{equation}
Similarly, it is clear that the optimal action value function is fixed point of this operator, while we can proof (in appendix \ref{appendix-2:optimal-q-bellman-contract}) that this operator is indeed a contraction mapping.

\subsubsection{SARSA}
All the algorithms presented till now are all model based algorithm, whereby we need a transition function $\mathcal{T}(s'|s, a)$ in order to solve MDP. This sometimes being intractable or unknown. From now on for the rest of the thesis, we will only consider the algorithm that doesn't require the knowledge of the environment, known as model-free. In model-free algorithm, we aim to estimate $Q^*(s, a)$ instead. That is because choosing action will be very easy if we get hold of $Q^*(s, a)$ simply choosing the action that maximizes the function given the state. Analogous to model based algorithm above, we want to update our current knowledge of $Q(s, a)$ and then do policy improvement on it following policy iteration, which we will call this SARSA. Before we move on, we would like to consider the general update rule at iteration $k$, which is 
\begin{equation}
    \label{eqn:train-q-val}
    Q^{(k)}(s, a) \leftarrow Q^{(k)}(s, a) + \alpha\brackb{\text{target value} - Q^{(k)}(s, a)}
\end{equation}
from now on, we will only consider about the target value. In addition, the experience collected will be of the form $(s_{t}, a_{t}, r_{t}, s'_{t}, a'_{t})$. SARSA depends on the concept called bootstrap, where by after we collect the experience, the target action value at iteration $k$ is
\begin{equation}
    r(s_t, a_t) + \gamma Q^{(k)}(s_{t+1}, a_{t+1})
\end{equation}
The follows the connection between value function and action value function, but ignoring the effect of expectation on policy and transition function. This will give an biased estimate of the action value function, although being low in variance. The bias estimate can be mitigated by including samples in $N$ later times, for instance 
\begin{equation}
    q^{(N+1)}_k = r(s_t, a_t) + \sum^N_{n=1} \gamma^n r(s_{t+n}, a_{t+n}) + \gamma^{N+1} Q^{(k)}(s_{t+N+1}, a_{t+N+1})
\end{equation}
With this, we can even do weighted sum on the N-steps estimation given hyperparameter $\lambda$ as follows
\begin{equation}
    (1-\lambda)\sum^\infty_{n=1} \lambda^{n-1} q^{(n)}_k
\end{equation}
which we will call forward view of SARSA($\lambda$). For an online implementation and other variances (eligibility traces), we refer to chapter 12 of \cite{sutton2018reinforcement}. 

\subsubsection{Q Learning}

For Q-learning, we simply aim to approximate optimal action value function. This can be done off-policy (meaning that any experience tuple sample from any policy will be sufficient). Similar to SARSA, which is based on policy evaluation, we based the algorithm on Q Iteration (Section \ref{sec:Q-iteration}) in which we can update the current estimate by setting the target value as:
\begin{equation}
    r(s_t, a_t) + \gamma \max_{a'} Q^{(k)}(s_{t+1}, a')
\end{equation}

\Phu{There is Michael Jordan's Paper on the convergence of this + Study the convergence of these stuff}

\subsection{Deep Reinforcement Learning}

Now, it is always the case that the state action value is too big for us to represent exact value function or action value function. We will now present the way that we can approximate these values with other method for control, notably policy gradient.

\subsubsection{Deep Q learning \cite{mnih2015human}}
Deep Q learning is based on traditional Q learning, whereby instead of update the Q function by adding the difference between target value and predicted value (see equation \ref{eqn:train-q-val}), the authors aimed to minimized mean square error of the action value function $Q_\theta(s, a)$ parameters by $\theta$. Furthermore, the training relies on experience replay $\mathcal{D}$, which stores the past experiences in the tuple (this works because Q learning is off-policy), and target action value function $Q_{\theta'}(s, a)$, which is the network some time before the current iteration, added to increase stability in training i.e
\begin{equation}
    \mathcal{L}(\theta) = \mathbb{E}_{(s, a, r, s')^n_{i=1} \sim B(\mathcal{D})}\brackb{ \frac{1}{n}\sum^n_{i=1} \bracka{Q_{\theta}(s_i, a_i) - r(s_i, a_i) - \max_{a'} Q_{\theta'}(s'_i, a'_i)}^2 }
\end{equation}
where $\mathcal{B}$ being mini-batch sampler. Furthermore, due to high number of state space, the author uses $\varepsilon$-greedy policy to help the agent collecting the experience (better exploration) so that it gets more diverse set of training data. The policy is defined as 
\begin{equation}
    \pi(a | s) = \begin{cases}
        \argmax{a'} Q(s, a') &\text{ if } b < \varepsilon \\
        a' \sim \text{Uniform}(a_0, a_1, \dots, a_{|A|}) &\text{ otherwise } \\
    \end{cases} \\
\end{equation}
where $b \sim \mathcal{U}[0, 1]$. The authors have shown that given the following training scheme (with some minor tricks), the agent can achieve super-human performance on Arcade Learning Environment \cite{bellemare2013arcade}. Note that there are some game that the agent doen't learn at all. The infamous Montezuma's revenge is one instance of this. This is due to the fact that the agent has to learn to reason on very long time steps in order to solve the game. This is solved via exploration and hierarchical reinforcement learning, which we will explore more in later sections.

There are other techniques that improves the training stability or increases its performance notable, double-q learning \cite{van2016deep}, which tackles the over-optimism prediction, Prioritized DQN \cite{schaul2015prioritized}, which allows the experience replay to sample higher quality experience tuple, Dueling DQN \cite{wang2015dueling}, which increase the capacity of the estimation of action value function by combining approximate value function and approximate advantage function, Distributional DQN \cite{bellemare2017distributional}, which estimates the distribution over the action value function \cite{osband2018randomized}, Noisy Net \cite{fortunato2017noisy}, which injects the noise into the output of neural network layers, aiming to improves explorations, and combination of them all - rainbow DQN \cite{hessel2018rainbow}.

\subsubsection{Policy Gradient}


\section{Extensions to control as inference frameworks}
After we have introduced control and inference framework, we will now present current progress of reinforcement learning made based on this framework. We will start with hierarchical agent, which is simplest extension to the framework. After this, we will look on how can be represents the unknown environment with belief over the state, which would requires us to formulate the problem into partially observable Markov decision process (POMDP). Finally, we will take a slight turn and examine current work on information theoretic inspired single agent reinforcement learning algorithms.

\section{Multi-agent reinforcement learning}
In this section, we will start formulating Multi-agent reinforcement learning problems, by first introducing a formulation of decentralized partially observable Markov decision process (DEC-POMDP), introducing some game theoretic concepts, with more traditional algorithms including Nash-Q \cite{hu2003nash}, MADDPG \cite{lowe2017multi}, COMA \cite{foerster2018counterfactual}, where some of them will be our baseline for our algorithms.

\section{Multi-agent with probabilistic inference framework}
Finally, we will present current algorithms developed by re-interpretation multi-agent reinforcement learning problem as control-as-inference framework. We will present 4 algorithms based on this: PR2 \cite{wen2019probabilistic}, ROMMEO \cite{tian2019regularized}, GR2 \cite{wen2019multi}, and Balancing-Q \cite{grau2018balancing}. All of them will be examined in full details because they lays a foundations to main result of this thesis. We will end this chapter by shinning light into incompatibility between these algorithms, which will solve in the later chapters. 