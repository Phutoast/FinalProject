\subsection{Policy Guarantee Improvement}
\label{appx:chap2-rl-policy-improve} 
We start with the definition of improved policy:
\begin{equation*}
    \pi'(a | s) = \begin{cases}
        1 &\text{ if } a = \argmax{a \in A} Q^\pi(s, a) \\
        0 &\text{ otherwise }
    \end{cases}
\end{equation*}
We have the following sequence of substitutions
\begin{equation*}
    \begin{aligned}
        V^{\pi}(s) &\le Q^{\pi}(s, \pi'(s)) \\
        &= r(s, \pi'(s)) + \gamma \mathbb{E}_{s' \sim \mathcal{T}(s' | s, a)}\brackb{V^\pi(s')}  \\
        &\le r(s, \pi'(s)) + \gamma \mathbb{E}_{s' \sim \mathcal{T}(s' | s, a)}\brackb{Q^{\pi}(s', \pi'(s'))}  \\
        &\vdots \\
        &\le r(s, \pi'(s)) + \gamma  r(s', \pi'(s')) + \gamma^2  r(s'', \pi'(s'')) + \cdots \\
        &= V^{\pi'}(s)
    \end{aligned}
\end{equation*}
Please note that this improved policy is deterministic, therefore, we can simply substitute the output into the action value function.

\subsection{Expected Bellman Operator is a Contraction Mapping}
\label{appx:chap2-rl-expected-bell-contract}
Recalling the definition of the Map:
\begin{equation}
    \contractop^{\pi} V(s) = \mathbb{E}_{a \sim \pi(a | s)} \brackb{r(s, a) + \gamma \mathbb{E}_{s' \sim \mathcal{T}(s' | s, a)}\brackb{V(s')}} 
\end{equation}
Let's consider the differences between 2 value functions $V_1(s)$ and $V_2(s)$ after being applied by the Bellman operator:
\begin{equation}
\begin{aligned}
    \contractop^\pi V_1(s) - \contractop^\pi V_2(s) &= \mathbb{E}_{a \sim \pi(a | s)} \brackb{ \cancel{r(s, a)} + \gamma \mathbb{E}_{s' \sim \mathcal{T}(s' | s, a)}\brackb{V_1(s')} - \cancel{r(s, a)} - \gamma \mathbb{E}_{s' \sim \mathcal{T}(s' | s, a)}\brackb{V_2(s')} } \\
    &= \gamma\mathbb{E}_{a, s'}\brackb{V_1(s') - V_2(s')}
\end{aligned}
\end{equation}
Now, we shall consider the $\infty$-norm of the equaltion, we have:
\begin{equation}
\begin{aligned}
    \max_{s}\left|\contractop^\pi V_1(s) - \contractop^\pi V_2(s)\right| &= \gamma\max_{s}\left|\mathbb{E}_{a, s'}\brackb{V_1(s') - V_2(s')}\right| \\
    &\le \gamma\max_{s}\left|\mathbb{E}_{a, s'}\brackb{\max_s\left|V_1(s') - V_2(s')\right|}\right| \\
    &\le \gamma \max_{s}\left| V_1(s) - V_2(s) \right| 
\end{aligned}
\end{equation}
Given this, we finished the proof.

% \subsection{Optimal value Bellman Operator is a Contraction Mapping}
% \label{appx:chap2-rl-optimal-val-bellman-contract}

% \subsection{Optimal action value Bellman Operator is Contraction Mapping}
% \label{appx:chap2-rl-optimal-q-bellman-contract}