\label{sec:chap2-other-algos}
This is the list of the algorithms that will be introduced in the following chapters: 
\begin{itemize}
    \item \hyperref[chapter:chap3]{Chapter 3}: Regularized Opponent Model with Maximum Entropy Objective (ROMMEO) \cite{tian2019regularized}, Probabilistic Recursive Reasoning (PR2) \cite{wen2019probabilistic}, and Balancing Q-learning \cite{grau2018balancing}.
    % \item \hyperref[chapter:chap5]{Chapter 5}\footnote{\hyperref[chapter:chap4]{Chapter 4} doesn't require any additional introduce but we assuming the readers have read \hyperref[chapter:chap3]{Chapter 3}}:  Maximum a Posteriori Policy Optimization (MPO) \cite{abdolmaleki2018maximum}, VIREL \cite{fellows2019virel}, and its on-policy variance V-MPO \cite{song2019v}. 
    \item \hyperref[chapter:chap6]{Chapter 6}: Soft Hierarchical reinforcement learning \cite{igl2019multitask, lobo2019soft} and Delayed communication or Dynamic policy termination Q-learning \cite{han2019multi}.
    % \item \hyperref[chapter:chap7]{Chapter 7}: Inverse reinforcement learning starting with Generative Adversarial Imitation Learning (GAIL) \cite{ho2016generative}, Adverserial Inverse Reinforcement Learning (AIRL) \cite{fu2017learning} with its connection between Generative Adversarial Network (GAN) \cite{goodfellow2014generative, finn2016connection}. Furthermore, we have unified view of adversarial imitation learning via $f$-GAN \cite{nowozin2016f} formulation \cite{ghasemipour2019divergence, ke2019imitation}. Also, we will also consider context awareness reinforcement algorithm \cite{yu2019meta, rakelly2019efficient}, and finally, multi-agent imitation learning including Multi-Agent GAIL (MAGAIL) \cite{song2018multi}, Multi-agent Adversarial Inverse Reinforcement Learning (MA-AIRL) \cite{yu2019multi} and, Decentralized Adversarial Imitation Learning algorithm with Correlated policies \cite{liu2020multi}.Last but not least the probabilistic interpretation of GAN, which includes Bayesian Generative Adversarial Imitation Learning \cite{jeon2018bayesian}, Importance weight GAN (IWGAN) \cite{hu2017unifying}, and $\alpha$-GAN \cite{rosca2017variational}, which is an extension of Adversarial autoencoders (AAE) \cite{makhzani2015adversarial}\footnote{As mentioned before, this chapter will be part-survey and part proposal to the new multi-agent imitation learning}.
    % \item \hyperref[chapter:chap8]{Chapter 8}: Generalized Recursive Reasoning (GR2) \cite{wen2019multi}
    \item \hyperref[chapter:chap9]{Chapter 9}: Variational Sequential Monte Carlo, Deep Variational Reinforcement Learning \cite{igl2018deep, shvechikovjoint} and Sequential Variational Soft Q-Learning (SVQN) \cite{huangsvqn}. They are the basis for representing belief, which is a basis for public belief MDP \cite{nayyar2013decentralized} and Bayesian Action Decoder (BAD) \cite{foerster2018bayesian}. 
\end{itemize}