\label{sec:chap2-deep-marl}

There are two types of algorithm used for training for multi-agent problems: centralized training and decentralized training algorithm. Centralized training algorithms are algorithms that required shared components between each agent to train, while the decentralized algorithm doesn't. Note that most of the algorithms here are centralized training algorithm. On the other hand, we can also follow single-agent training algorithms classification, which separates value-based algorithms (for example DQN \cite{mnih2015human}) from policy-based algorithms (for example, REINFORCE \cite{sutton2000policy} and A2C \cite{mnih2016asynchronous}). 

\subsection{Policy Based}

\subsubsection{Counterfactual Multi-Agent Policy Gradients (COMA-PG) \cite{foerster2018counterfactual}}
We consider solving cooperative multi-agent problems where the main problem comes from the fact that it is difficult for the training algorithm to assign correct credit to correct agent. COMA-PG \cite{foerster2018counterfactual} solved this problem by proposing the following baseline for a policy gradient algorithm:
\begin{equation}
    D^i = r(s, a) - r(s, (a_c^i, a^{-i}))
\end{equation}
where $a_c$ is the \textit{default action} for agent $i$. The default action can be, simply, an expected action, so now the advantage function becomes:
\begin{equation}
    A^i(s, a) = Q(s, a) - \sum_{a'_i} \pi^i(a'_i | s) Q(s, (a'_i,a^{-i}))
\end{equation}
We can now use it to train a normal policy gradient algorithm.

\subsubsection{Multi-Agent DDPG (MADDPG) \cite{lowe2017multi}}
Now, we can consider the policy gradient for any multi-agent problem as, where we can treat a joint actions $(a_1, \dots, a_N)$ as one action from one agent:
\begin{equation}
\nabla_{\theta_i}J(\theta_i) = \mathbb{E}_{s\sim d(s), a_i \sim \pi_i} \left[ \nabla_{\theta_i} \log \pi_i(a_t | s_i) Q^{\pi}_i(s, a_1, \dots, a_N) \right]
\end{equation}
Now, the authors extended this to deterministic policy (similar to DDPG, see section \ref{sec:chap2-ddpg} for more details) $\mu_{\theta_i}$, which is defined as 
\begin{equation}
    \nabla_{\theta_i}J(\mu_{\theta_i}) = \mathbb{E}_{s, a \sim \mathcal{D}} \left[ \nabla_{\theta_i} \mu_{\theta_i}(a_i | s_i) \nabla_{a_i} Q^{\mu}_i(s, a_1, \dots, a_N) |_{a_i = \mu(s_i)} \right]
\end{equation}
where $\mathcal{D}$ is experience replay that contains a tuple $(s, s', a_1, \dots, a_N, r_1, \dots, r_N)$, and, finally, the critic can be trained by minimizing the following objective 
\begin{equation}
    \mathcal{L}(\theta_i) = \mathbb{E}_{(s, a, s', r) \sim \mathcal{D}}\left[ \left( Q^{\mu}_i (s, a_1, \dots, a_N) - r_i - \gamma Q^{\bar{\mu}}_i(s', a'_1, \dots, a'_N) |_{a'_i = \bar{\mu}(s_i')} \right)^2  \right]
\end{equation}
There is a notion of \textit{centralized} as we have to have a full knowledge of other agent's policy, while \textit{decentralized}\footnote{This is what we aim for} is where we doesn't assume any knowledge of other agent's inner policy, but we can still observe their actions. 

\subsection{Value Function Based}
Now we have seen the use of value estimation in MADDPG; let's consider the method of centralized value function estimation. Please note that all the algorithms in this section assume a cooperative setting. The main intuition behind all value function based algorithm is that the action value function Q can be decomposed to each agent's value. In value decomposition network (VDN) \cite{sunehag2017value}, the authors assume the following factorization of the agent:
\begin{equation}
\begin{aligned}
Q(s (a^1, \dots, a^N)) \approx \sum^N_{i=1} \tilde{Q}_i(s, a^i)
\end{aligned} 
\end{equation}
Note that this works nicely in the case of cooperative setting as in competitive game the value function mixing is likely to be more \textit{complex} interactions, in contrast to, cooperative as they are likely moving toward the same goal. QMIX \cite{rashid2018qmix} is an extended version of VDN, by including a mixing network, instead of using the summation. However, there has to be some constrain on the function. Notably, 
\begin{equation}
\underset{a}{\arg\max} \ Q_{\text{tot}}(s, a) \ = \begin{pmatrix}
	\underset{a}{\arg\max} \ Q_1(s, a) \\ \underset{a}{\arg\max} \ Q_2(s, a) \\ \vdots \\ \underset{a}{\arg\max} \ Q_N(s, a) \\
\end{pmatrix}
\end{equation}
The VDN satisfies the condition but there is a more generalized notion of this, which is monotonicity.
\begin{equation}
\forall n \in [N] : \frac{\partial Q_{\text{tot}}}{\partial Q_{n}} \ge 0
\end{equation}
A monotonic function can be represented relatively well \cite{dugas2009incorporating} by making the function positive ($\ge 0$), which we can use ReLU function \cite{nair2010rectified} to satisfies the condition. The training is standard Q-value objective. Finally, as shown in \cite{mahajan2019maven}, QMIX can suffer from learning sub-optimal action-value function with poor exploration. To prevent this from happening, MAVEN \cite{mahajan2019maven} learns multiple modes of value functions given latent variable $z$, with the following process
\begin{itemize}
    \item The value function is conditioned on shared latent variables $z$ which is controlled by hierarchical policy $z = g_\theta(x, s_0)$ where $x$ is value that is sampled from simple distribution $P(x)$
    \item For a given $z$, each joint action-value function is monotonic approximation of optimal action value function $g_{\eta}^a(o^a_{1:t}, a_{1:t}, u^{a}_{1:t-1})$ together with $g_{\phi}(z, a)$ that is a neural network that modify the utility for particular mode of exploration. Now all value function are mixed using $g_{\psi}$ function 
\end{itemize}
We will train the Q-network as following 
\begin{equation}
    \mathcal{L}_{Q}(\phi, \eta,\psi) = \mathbb{E}_{\pi_a}  \left[ Q(u_t, s_t ; z) - \left[r(u_t, s) + \gamma \max_{u_{t+1}} Q(u_{t+1}, s_{t+1} ; z)\right] \right]
\end{equation}
while the hierarchical policy is trained cumulative rewards $R(\tau, z | \phi, \eta, \psi)= \sum_t r_t$ 
\begin{equation}
    \mathcal{L}_{RL}(\theta) = \int R(\tau_A | z)P_{\theta}(z | s_0) \ \dby z\dby s_0
\end{equation}
To prevent collapse of the latent variable $z$, the mutual information objective for a trajectory $\tau = \{(u_t, s_t)\}$ is introduced where we need $\sigma$ which returns Boltzman policy given the action-value function, while the trajectory is encoded using RNN. Now, the mutual information loss is lower bounded by
\begin{equation}
\begin{aligned}
    \mathcal{L}_{ML} &= \mathcal{H}(\sigma(\tau)) - \mathcal{H}(\sigma(\tau) | z) = \mathcal{H}(z) - \mathcal{H}(z | \sigma(z)) \\
    &\ge \underbrace{\mathcal{H}(z) + \mathbb{E}_{\sigma(z), z} \left[ \log q_v(z | \sigma(\tau)) \right]}_{\mathcal{L}_v(\phi, \eta, \psi, v)}
\end{aligned}
\end{equation}
which leads to an auxiliary reward as $r^z_{\text{aux}} (z) = \log(q_v(z | \sigma(z))) - \log  (p(z))$. The final objective is 
\begin{equation}
    \max_{v, \phi, \eta, \psi, \theta} \mathcal{L}_{RL}(\theta) + \lambda_{MI}\mathcal{L}_v(\phi, \eta, \psi, v) - \lambda_{Q}\mathcal{L}_{Q}(\phi, \eta,\psi)
\end{equation}