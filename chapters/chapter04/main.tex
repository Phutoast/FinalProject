\label{chapter:chap4}
\begin{miniabstract}
After we have a unified view on MAPI framework, which we have reinterpreted Balancing Q-learning \cite{grau2018balancing} as probabilistic inference. We are now going to consider extensions to the agent's functionality. In this chapter, we are going to consider the case of temporal abstraction and communication in a cooperative multi-agent reinforcement learning problem. We will start by finding single-agent option learning problem and solving it together with soft actor-critic like algorithm, as a minor contribution. After considering the single-agent problem, we are going to define a model in which the agent can plan and communicate and solving in a multi-agent setting.
\end{miniabstract}


\section{Single Agent Soft-Hierarchical Reinforcement Learning}
\input{chapters/chapter04/contents/01_single_agent_HRL}

\section{Multi-Agent Delayed Soft-Option Communication}
\input{chapters/chapter04/contents/02_multi_agent_comunication}

\section{Implementation}
\input{chapters/chapter04/contents/03_implementations}

