\label{sec:chap6-summary}
Now, we have shown the potential of control as probabilistic inference framework in multi-agent reinforcement learning. We start from expanding the training capability of the current framework by reinterpreting Balancing-Q learning into a probabilistic framework. Then we move to expanding agent's capability so that it is able to do temporal abstracting with communication and reasoning in an unknown state via public-belief MDP framework. 

The ability of this framework lies in the fact that we can represent our problem as a graphical model, which leads directly into the answer. By utilizing tools from single agent reinforcement, the algorithm developed is sounded. 

Finally, the aim of this thesis is to proposed one of the ways multi-agent reinforcement learning problem can be tackled systematically via the understanding of the problem and interpreting it as a probabilistic model. 