\subsection{Control as Probabilistic Inference}
\label{sec:chap1-MERL-intro}
We shall start with a probabilistic framework for single-agent problems before we even jump into the issue of multi-agent problems. Control as probabilistic inference framework \cite{levine2018reinforcement, ziebart2008maximum}, just like other probabilistic inference problem, is trying to infer some distribution, which in this case $P(a | s, \optim)$, where, we have, $a$ that represents action, $s$ that represents state and $\optim$ that represents the \textit{optimality} of the action and the state of the agent. The optimality random variable is an observable variable defined by 
\begin{equation}
    P(\optim | s, a) \propto \exp\bracka{\beta r(s, a)}
\end{equation}
while the agent's state $s$ and action $a$ are treated as latent/unseen variable. Please, pay attention to a small variable $\beta$ as it will play a crucial role in the future, where it is the variable that determines the \correctquote{rationality} of the agent\footnote{Rationality is defined to be the agent's ability to gain more rewards.}. However, inference such a distribution can be intractable. We will have to approximate the distribution $P(a | s, \optim)$ by another parameterized distribution $q_\theta(a | s)$, which we call it a policy/variational distribution. For more details on probabilistic inference, please see chapter \ref{chapter:chap2}. We indeed have turned a probabilistic inference problem into an optimization problem, which in this case, our optimization objective is similar to a normal single-agent reinforcement learning problem (where we maximize the sum of reward\footnote{For details on non-regularized reward maximization problem, please see section \ref{sec:chap2-single-rl} in chapter \ref{chapter:chap2}.}) with additional regularization on the agent policy
\begin{equation}
\label{eqn:chap1-optim-objective}
    \max_q \ \mathbb{E}_{q(\tau)}\Bigg[\sum^T_{t=1} \beta r(s_t, a_t) - \log\frac{q(a_t | s_t)}{P_\prior(a_t | s_t)} \Bigg]
\end{equation}
where $q(\tau)$ is the probability of a state action trajectory that is induced by the agent acting in an environment. The regularization term is called Kullback-Leibler divergence (KL-divergence) \cite{kullback1951information, sherman1960solomon}, which measure the \correctquote{distance}\footnote{KL-divergence is non-symmetric therefore isn't \textit{technically} a distance.} between 2 probability distributions. The optimization problem from the probabilistic inference framework leads us to the problem of maximizing total reward while maintaining $q(a_t | s_t)$ to be closer to our \correctquote{prior} on the agent's policy. If the prior distribution is uniform, then regularization in the optimization problem will turn into entropy regularization, hence the name maximum entropy reinforcement learning (MERL). Entropy is associated with the \correctquote{randomness} of the probability distribution, meaning that agent will be encouraged to be as random as possible while being able to gain higher rewards, this behaviour helps the agent to explore the environment. Furthermore, it is easier when implementing using an entropy regularization, since we don't have to do any engineer on the prior, which is the reason why people usually refer to the framework as \correctquote{maximum entropy} because they use it more and partly due to historical reason. However, KL-divergence will be useful for us in multi-agent setting. To reduce the confusion, we will call the framework \textit{control as probabilistic inference} but sometimes will use the term \correctquote{MERL}, interchangeably. 

Before we move to practical algorithms, let's provide a simple explanation of $\beta$. When $\beta$ closer to zero, the agent will ignore the reward while paying more attention to the regularization of the policy $q(a_t | s_t)$ (forcing it to be closer to the prior). By making the policy closer to the prior, we can reduce in the rationality of the agent because we set $\beta$ to be zero the optimal agent according to our objective is the prior policy, a baseline. We can select $\beta$ in such a way that provides balance between the complexity of the agent and reward gained. This objective can be a representation of \correctquote{bounded rationality}. Finally, this process can be interpreted in many ways via Ockham razor and Information theory \cite{ortega2013thermodynamics, tim2015bounded}.

There are two practical algorithms that we can choose: either to optimize this objective directly via policy gradient method\footnote{See section \ref{sec:chap2-ator-critic} in chapter \ref{chapter:chap2} for more details on the method.} \textit{or} finding the closed-form solution. Since it is usually intractable in continuous action space, we, then, trying to approximate the solution. The policy gradient method can be simple, as we can achieve via auto-differentiation and we, therefore, will not go into further details. Now, for the second option, the closed-form solution is:\footnote{We will provide more details of the derivation and further interpretation in section \ref{sec:chap2-derivation-soft-closed-form} in chapter \ref{chapter:chap2}. The derivation itself is very important and will be used throughout the text.}:
\begin{equation}
\begin{aligned}
    &\pi(a_t | s_t) = \frac{P_\prior(a_t | s_t) \exp\bracka{Q(s_t, a_t)}}{\exp\bracka{V(s_t)}} \\
    \text{ where }& V(s_t) = \log \int P_\prior(a_t | s_t) \exp\bracka{Q(s_t, a_t)} \dby a_t \\
    &Q(s_t, a_t) = \beta r(s_t, a_t) + \gamma\mathbb{E}_{s_{t+1}}\brackb{V(s_{t+1})}
\end{aligned}
\end{equation}
Since $Q(s_t, a_t)$ can be represented by a complex function approximator, such as neural network, then finding $V(s_t)$ would be intractable. However, some algorithms can approximate these terms and the agent's policy, mainly soft Q-learning \cite{haarnoja2017reinforcement} and soft actor-critic \cite{haarnoja2018softa, haarnoja2018softb}. In conclusion, we can turn the inference problem into more simple optimization problem via approximation of intractable quantity. The approximation procedure would be a recurrent theme of not only this work but also the majority of the development of machine learning algorithms.  

\subsection{MEMARL framework and Its problem}
\label{sec:chap1-MEMARL-And-Problem}
We can divide maximum entropy multi-agent reinforcement learning (MEMARL) algorithms into 2 sub-types: 
\begin{enumerate}
    \item Multi-agent learning as probabilistic inference (MAPI), which includes Probabilistic Fictitious Play \cite{rezek2005unifying}, Probabilistic Recursive Reasoning (PR2) \cite{wen2019probabilistic} , Regularized Opponent Model with Maximum Entropy Object (ROMMEO) \cite{tian2019regularized} and Generalized Recursive Reasoning (GR2) \cite{wen2019multi}
    \item Multi-agent learning with KL-constraint (MAKL), which only includes Balancing Q-learning \cite{grau2018balancing}
\end{enumerate}
Optimization objective for them is all in a similar form, which is reward and KL-regularization. For instance, the objective for PR2 is 
\begin{equation}
    \mathbb{E}\left[ \sum^T_{t=0} \gamma^t \left(\beta r(s_t, a_t, a^{-i}_t) - \log\frac{\pi(a_t|s_t)}{P_{\operatorname{prior}}(a_t|s_t)} -\log \frac{\rho(a^{-i}_t|s_t, a^i_t)}{P_{\operatorname{prior}}(a^{-i}_t|s_t, a^i_t)} \right) \right]
\end{equation}
while the objective for Balancing Q-learning is 
\begin{equation}
    \mathbb{E}\left[ \sum^T_{t=0} \gamma^t \left(r(s_t, a_t, a^{-i}_t) - \frac{1}{\beta^i} \log\frac{\pi(a_t|s_t)}{P_{\operatorname{prior}}(a_t|s_t)} - \frac{1}{\beta^{-i}}\log \frac{\rho(a^{-i}_t|s_t)}{P_{\operatorname{prior}}(a^{-i}_t|s_t)} \right) \right]
\end{equation}
The difference between MAPI and MAKL framework is how they arrived at the final algorithm. On one hand, MAPI derivation is based on the principled control as probabilistic inference framework\footnote{We will provide derivations of some algorithms in section \ref{sec:chap3-intro-MAPI} in chapter \ref{chapter:chap3}} by trying to find the posterior of $P(a^i, a^{-i} | s, \optim^i_t = 1, \optim^{-i}_t = 1)$ where
\begin{equation}
    P(\optim^i_t = 1 | \optim^{-i}_t = 1, s_t, a_t, a^{-i}_t) \propto \exp\left( \beta r(s, a^i, a^{-i}) \right)
\end{equation}
where the difference between each algorithm in the family is how the agents' policy is factorized, in ROMMEO case, we assume the agent is best responding to opponents' action (left side of equation \ref{eqn:chap1-factorize-PR2-ROMMEO}). In contrast, in PR2 case the opponent's action is based on the agent's action, which we can see as agent learning to \textit{manipulate} its opponent (right side of equation \ref{eqn:chap1-factorize-PR2-ROMMEO}).
\begin{equation}
\label{eqn:chap1-factorize-PR2-ROMMEO}
    P(a^{i}, a^{-i} | s) = \pi(a^{i} | s, a^{-i}) \rho(a^{-i} | s) \qquad P(a^{i}, a^{-i} | s) = \pi(a^{i} | s) \rho(a^{-i} | s, a^{i})
\end{equation}
On the other hand, MAKL takes inspiration from the \textit{result} (not the derivation) of control as inference framework, and a related concept of bounded rationality as they explicitly construct Balancing Q-learning from KL-constraint without the use of inference techniques \cite{grau2018balancing}. 

However, the problem that Balancing Q-learning is trying to solve, and its solution can't be mapped to the current toolbox that MAPI employed, which is when two agents shouldn't be cooperative. We will not associated MAKL with how the joint actions\footnote{\textit{Spolier} we can map Balancing Q-learning to the probabilistic framework, and it is a PR2-like factorization. This finding would be the main result of chapter \ref{chapter:chap3}, which will be in section \ref{sec:chap3-balancing-Q-derivation} and the insight would drive the result of at the end of the chapter.} are factorized because there is no point comparing an apple to a red pear\footnote{Of course, this is a reference to \correctquote{comparing apple to orange} but even if these 2 fruits have the same colour, it doesn't mean we should compare then !}. So, how can Balancing Q-learning achieve both cooperative and competitive setting? The answer lies in the $\beta$s. If both of then have opposite sign, then we have a competitive setting, and if both have the same sign, then we have a cooperative setting. Note that setting negative $\beta$ in the single-agent case would make the agent trying to avoid any actions that would maximize the reward. An intuitive explanation is that $\beta$ acts as a mask that changes the agents' perception of the reward. The problem with current MAPI is that we can't force each agent \footnote{We left the notion of opponent model for the sake of simplification, but the argument still holds} to perceives reward without a common mask or $\beta$. 

In summary, MAPI provides us with a principled approach to develop multi-agent reinforcement algorithms. However, it lacks expressiveness to solve the competitive game. On the other hand, MAKL gives us a baked formulation that allows us to works on a competitive game, which can be a difficulty when we want to extend the current MEMARL framework. We will need to find a way to develop an algorithm that is similar to one derived from MAKL, but is based on MAPI approach, to expand the horizon of multi-agent algorithms development that is based on probabilistic inference, which is one of our goals of this thesis.

\subsection{Plausible extensions}
\label{sec:chap1-MEMARL-Extensions}
Now, we have seen the problems regarding the probabilistic inference algorithm. In this section, we are going to provides the reason \textit{why} we should resolve the problem. We argue that the control as probabilistic inference has shown to be very fruitful in reinterpreting some of the reinforcement learning algorithms. In this thesis, we consider two crucial extensions to multi-agent reinforcement learning. This includes: hierarchical reinforcement learning (option based) with communication and representing/inferring hidden state in partial observable Markov decision process (POMDP) setting. We choose both aspects because we consider the ability to communicate and inferring hidden information to be important feature any multi-agent should have. 

\paragraph{Hierarchical Control with Delayed Option Communication:} 
Our motivation starts from the option framework as we want agents to be able to control with temporal abstraction \cite{sutton1999between, bacon2017option}. The setting we are having consists of a master/higher-level agent\footnote{We will use these terms interchangeably} that emits a new option $o_t$ when the termination policy decided to terminate the option beforehand $o_{t-1}$ by setting $b_t = 1$. The option will conditions to lower-agent action, which associate with real environment action i.e $\pi(a_t | s_t, o_t)$. We can interpret $o_t$ as the mode of action that the agent can execute. This framework has been reinterpreted to probabilistic inference problem in \cite{igl2019multitask} however, it lacks closed formed solution which would allow us to apply soft actor-critic (or soft Q-learning) like algorithm. As a minor contribution, we will try to solve the probabilistic option framework in single-agent fully case\footnote{There has been an attempt in solving it \cite{lobo2019soft}. Still, there are some mistakes with the derivation, as mentioned by the authors. We will point out the error and provide the solution}. After this, we move on to the multi-agent case, which we can fully derive the probabilistic version, which leads, ultimately to, delayed option communication framework proposed in \cite{han2019multi}. 


% \paragraph{Imitation learning with Context Awareness: } There has been a significant development in maximum entropy inverse reinforcement learning \cite{ziebart2008maximum} with exciting works on adversarial imitation learning \cite{ho2016generative, fu2017learning, ghasemipour2019divergence} with its connection to generative adversarial network (GAN) \cite{goodfellow2014generative, finn2016connection}. There are also works \cite{petangoda2019disentangled, rakelly2019efficient, yu2019meta} in which the authors tries train the agent to infer the \correctquote{type} of the environment represented in latent variable $z_t$ based on its past experience, so that the generalized policy would act correctly conditioned to the context of the environment $\pi(a_t | s_t, z_t)$ leading to plausible multi-task/meta-learning algorithms. We envisioned the agent to be able to infer others' goals and its value function, which is compressed in single latent variable $z_t^{-i}$\footnote{or in anthropomorphic terms: \textit{personality} (we disagree to use such a term but this might help the reader to understand the motivation behind and the kind of task, we try to solve).} so that it can be robust once encountering opponent that isn't appear in training i.e $\pi(a_t^i | s_t, z^{-i}_t)$. Before, we can reach such model, there are difference algorithms with imitation learning multi-agent reinforcement learning \cite{song2018multi, yu2019multi, liu2020multi} however, all of the models aim to imitate the dataset of trajectories, while our goal is to utilized this framework as an opponent model so that the agent can \textit{benefit}\footnote{This means we want our agent to exploit the knowledge about its opponents that it has and not just imitate} from it. This can be done by exploiting the training procedure, which reduces the problem into single agent problem, while being faithful to control as probabilistic framework. As a minor contribution, We also going to survey multiple single agent imitation learning algorithm as a possible extensions to the multi-agent adversarial framework.

\paragraph{Representing Belief:} In a partially observable setting (either in a single-agent case or a multi-agent case), the agent has to form a belief over the states based on history local observations or information that is received. In single-agent case, we normally use a recurrent neural network (RNN) as the tools to aggregated this local information and form a belief. However, they are not \correctquote{probabilistic} in the sense that there isn't any uncertainty associated with the state representation since RNN only compress the history into a single variable. There have been developments in representing a belief over state in single-agent domain. One of the algorithms is called \textit{Deep variational reinforcement learning} (DVRL) \cite{igl2018deep} that utilized of variational sequential Monte Carlo (VSMC) \cite{le2017auto, maddison2017filtering, naesseth2017variational}, an extension of algorithms called sequential Monte Carlo \cite{doucet2001introduction} by parameterized the transition probability and emission probability that operate on hidden Markov model. DVRL allows an agent to jointly learn the model of the environment and maximize its reward. There has been a variance of DVRL \cite{shvechikovjoint}, which provides some insight into how similar algorithms can be derived via control as probabilistic inference framework. In a nutshell, VSMC maintains a set of particles, where each of them is a tuple of 2 elements: state representation $s_t$ and its associated weight $w_t$\footnote{We have simplified the description, but the arguments still hold. In DVRL, the authors assign particle as a three elements tuple: $(z_t, h_t, w_t)$ where $z_t$ is latent state representation, $h_t$ is the hidden representation of RNN, and $w_t$ is the weight of each particle, which acts as uncertainty quantifier.}, which is updated every time the agent acts given its local information. There has been works done in employing sequential Monte Carlo method in multi-agent case \cite{doshi2009monte} via Interactive-Partial observable Markov decision process (I-POMDP) formulation \cite{gmytrasiewicz2005framework}. The biggest problem with I-POMDP is that modelling nested belief can be computationally expensive. Given a cooperative setting, we can, alternatively, consider a public belief MDP framework proposed in \cite{nayyar2013decentralized}, which has been successfully scaling up via approximation and neural network in \cite{foerster2018bayesian, hu2019simplified}. The public belief MDP reduces multi-agent problem into POMDP problem and remove nested belief consideration in I-POMDP, which carves the way for us to apply control as probabilistic inference framework to a multi-agent problem. 

% \footnote{We are aware of the work done in \cite{huangsvqn} however they didn't employ any importance weighting or particle filter that would make the variational bound tighter (and a representation of uncertainty), which is the main aim of DVRL and ours, while \cite{shvechikovjoint} results resemblance \cite{igl2018deep} much closer. We will discuss the differences in section in chapter \ref{chapter:chap9}}

