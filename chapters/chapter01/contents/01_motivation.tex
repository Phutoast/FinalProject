\subsection{Control as Probabilistic Inference}
\label{sec:chap1-MERL-intro}
We shall start with single agent framework first, before we even jump into the problem of multi-agent framework. Control as probabilistic inference framework \cite{levine2018reinforcement}, just like other probabilistic inference problem, trying to calculate certain distribution, in this case it is $P(a | s, \optim)$, where $a$ represents action of the agent, $s$ represents states that the agent is in and $\optim$ represent the \textit{optimality} of the action and the state of the agent. The optimality random variable is an observable variable defined by 
\begin{equation}
    P(\optim | s, a) \propto \exp\bracka{\beta r(s, a)}
\end{equation}
while the agent's state $s$ and action $a$ are treated as latent variable. Please, remember a small variable $\beta$ as it will play a crucial role in the future as it is the variable that determines the \correctquote{rationality} of the agent\footnote{See chapter \ref{chapter:chap3} and chapter \ref{chapter:chap4} for greater details}. However, the \correctquote{inference}  can be intractable, in which we have to approximate the distribution $P(a | s, \optim)$ by another simpler distribution $q(a | s)$, which may be parameterized and we called it agent's policy. For more details on probabilistic inference please see chapter \ref{chapter:chap2}. We indeed have to turn the probability problem into optimization problem, which in this case, our optimization objective is similar to single agent problem (where we only trying to maximize the reward\footnote{For details on non-regularized reward maximization problem please see section in chapter \ref{chapter:chap2}.}) with additional regularization on the agent policy
\begin{equation}
\label{eqn:chap1-optim-objective}
    \max_q \ \mathbb{E}_{q(\tau)}\brackb{\sum^T_{t=1} \beta r(s_t, a_t) - \log\frac{q(a_t | s_t)}{P_\prior(a_t | s_t)} }
\end{equation}
where $q(\tau)$ is the probability of trajectory that is induced by the agent. The regularization term, including the negative terms, is actually called Kullback-Leibler divergence (KL-divergence), which measure the \correctquote{distance}\footnote{KL-divergence is non-symmetric therefore isn't \textit{technically} a distance.} between 2 probability distributions. This means that we want the reward to be maximized while maintaining $q(a_t | s_t)$ to be closer to our \correctquote{prior} belief on the agent's policy. Suppose the prior is uniform, then regularization in the optimization problem (equation \ref{eqn:chap1-optim-objective}) will turns to entropy regularization, hence the name maximum entropy reinforcement learning (MERL). Entropy is associated with the \correctquote{randomness} of the probability distribution, and the optimization procedure will make the agent to be as random as possible while being able to gain higher rewards, which in turn will help agent to explore. It is easier when implementing using an entropy regularization, since we don't have to do any engineer on the prior, which is the reason why people usually refer to it as \correctquote{maximum entropy} (because they use it more). However, KL-divergence will be useful for us in multi-agent setting. To reduce the confusion, we will call the framework \textit{control as probabilistic inference} but sometimes will use MERL and MKLRL (minimum KL reinforcement learning) interchangeably. 

Before we move to a more practical algorithms, let's provide a simple explanation of $\beta$. When $\beta$ closer to zero, the agent will ignores the reward while paying more attention to the regularization of the policy $q(a_t | s_t)$ (to be closer to the prior). By making the policy closer to the prior than maximizing the reward, which we can call a reducing in the rationality of the agent, $\beta$ can be select in such a way to provide a balance between the complexity of the agent and the reward gained\footnote{We refer to section in chapter \ref{chapter:chap2} for more perspective of this regularization terms}. 

For the practical solution to this optimization problem, there are 2 choices that we can choose: either to directly optimize this objective directly via policy gradient method\footnote{See section in chapter \ref{chapter:chap2} for more details on the derivation.} \textit{or} finding the closed formed solution if it is intractable (spoiler: it is) then trying to approximate it. The policy gradient method can be simple as we can achieved via auto-differentiation and we, therefore, will no goes into further details. Now, for the second option, we can derived the agent in a closed form as follows\footnote{We will provide more details of the derivation and further interpretation in section in chapter \ref{chapter:chap2}. The derivation itself is very important and will be used throughout the text.}:
\begin{equation}
\begin{aligned}
    &\pi(a_t | s_t) = \frac{P_\prior(a_t | s_t) \exp\bracka{Q(s_t, a_t)}}{\exp\bracka{V(s_t)}} \\
    \text{ where }& V(s_t) = \log \int P_\prior(a_t | s_t) \exp\bracka{Q(s_t, a_t)} \dby a_t \\
    &Q(s_t, a_t) = \beta r(s_t, a_t) + \gamma\mathbb{E}_{s_{t+1}}\brackb{V(s_{t+1})}
\end{aligned}
\end{equation}
Now, if $Q(s_t, a_t)$ is represented by a complex function approximator such as neural network then finding $V(s_t)$ would be intractable. However, there are algorithms that are able to approximate these terms and the agent's policy, mainly soft Q-learning \cite{haarnoja2017reinforcement} and soft actor-critic \cite{haarnoja2018softa, haarnoja2018softb}\footnote{ibid.}. Now, we have briefly explore the control as probabilistic inference framework. In conclusion, we can see that we can turn the inference problem into more simple optimization problem via approximation of intractable quantity. This would be a recurrent theme of not only this work but majority of the development of machine learning algorithms.  

\subsection{MEMARL framework and Its problem}
\label{sec:chap1-MEMARL-And-Problem}
We can divide maximimum entropy multi-agent reinforcement learning (MEMARL) algorithms into 2 main types: 
\begin{enumerate}
    \item Multi-agent learning \textit{from} probabilistic inference (MAPI): Probabilistic Fictitious Play \cite{rezek2005unifying}, Probabilistic Recursive Reasoning (PR2) \cite{wen2019probabilistic} , Regularized Opponent Model with Maximum Entropy Object (ROMMEO) \cite{tian2019regularized} and Generalized Recursive Reasoning (GR2) \cite{wen2019multi}
    \item Multi-agent learning \textit{from} KL-constraint (MAKL), which only includes Balancing Q-learning \cite{grau2018balancing}
\end{enumerate}
Optimization objective for them are all in the same form, some kind of reward and KL-regularization for the agents. For instance, the objective in PR2 is 
\begin{equation}
    \mathbb{E}\left[ \sum^T_{t=0} \gamma^t \left(\beta r(s_t, a_t, a^{-i}_t) - \log\frac{\pi(a_t|s_t)}{P_{\operatorname{prior}}(a_t|s_t)} -\log \frac{\rho(a^{-i}_t|s_t, a^i_t)}{P_{\operatorname{prior}}(a^{-i}_t|s_t, a^i_t)} \right) \right]
\end{equation}
while the objective for Balancing Q-learning is 
\begin{equation}
    \mathbb{E}\left[ \sum^T_{t=0} \gamma^t \left(r(s_t, a_t, a^{-i}_t) - \frac{1}{\beta^i} \log\frac{\pi(a_t|s_t)}{P_{\operatorname{prior}}(a_t|s_t)} - \frac{1}{\beta^{-i}}\log \frac{\rho(a^{-i}_t|s_t)}{P_{\operatorname{prior}}(a^{-i}_t|s_t)} \right) \right]
\end{equation}
The difference between MAPI and MAKL algorithms is how they arrived at the final algorithm. On one hand, MAPI derivation is based on the principled control as probabilistic inference framework\footnote{We will provide derivations of some algorithms in section  in chapter \ref{chapter:chap3}} by trying to find the posterior of $P(a^i, a^{-i} | s, \optim)$ where\footnote{We didn't use the same optimality condition that are presented in ROMMEO (see equation 5 in \cite{tian2019regularized}). There is a reason to it, which will be explained in section in chapter \ref{chapter:chap3}} 
\begin{equation}
    P(\mathcal{O} = 1) \propto \exp\left( \beta r(s, a^i, a^{-i}) \right)
\end{equation}
where the differences between each algorithms in the famility is how the agents' policy is factorized; in ROMMEO case, we assume the agent is best responding to opponents' action (left side of equation \ref{eqn:chap1-factorize-PR2-ROMMEO}), while in PR2 case the opponent's action is based on the agent's action, which we can see as agent learning to \textit{manipulate} its opponent (right side of equation \ref{eqn:chap1-factorize-PR2-ROMMEO}).
\begin{equation}
\label{eqn:chap1-factorize-PR2-ROMMEO}
    P(a^{i}, a^{-i} | s) = \pi(a^{i} | s, a^{-i}) \rho(a^{-i} | s) \qquad P(a^{i}, a^{-i} | s) = \pi(a^{i} | s) \rho(a^{-i} | s, a^{i})
\end{equation}
On the other hand, MAKL takes inspiration from the \textit{result} (not the derivation) of control as inference framework, and a related concept of bounded rationality as they explicity construct Balancing Q-learning from KL-constraint without any mension of inference algorithms \cite{grau2018balancing}. However, the problem that MAKL trying to solve and its solution can't be mapped to the current toolbox that MAPI employed, which is when 2 agents shouldn't cooperative. We will not associated MAKL with how the joint actions\footnote{\textit{Spolier} we can indeed map Balancing Q-learning to probabilistic framework and it is a PR2-like factorization. This would be a main result of chapter \ref{chapter:chap3}, which wil be in section  and the insight would drive the result of chapter \ref{chapter:chap4}.} are factorized because there is no point comparing an apple to a red pear\footnote{Of course, this is a reference to \correctquote{comparing apple to orange} but even if these 2 fruits have the same colour, it doesn't mean we should compare then !}. So, how can Balancing Q-learning achieves both cooperative and zero-sum setting ? The answer lies in the $\beta$s. If both of then have opposite sign then we have a zero-sum setting, and if both have same sign then we have a cooperative setting. Note that setting negative $\beta$ in single agent case would make the agent trying to avoid any actions that would maximized the reward. An intuitive explanation is that $\beta$ acts as a mask that changes the agents' perception on the reward. Furthermore, this also apply on to the fact that if $\beta$ is near zero then the reward will not have any impact on the agent's final behavior.  The problem with current MAPI is that we can't force the each agents\footnote{We left the notion of opponent model for the sake of simplification, but the argument still holds} to perceives reward without common mask or $\beta$. 

In summary, MAPI provides us with principled approach to develop multi-agent reinforcement algorithms, however, it lacks expressiveness to solve zero-sum game. On the other hand, MAKL gives us with baked formulation that allows us to works on zero-sum game, which can be a difficulty when we want to extends the current MEMARL framework. We will need to find a way so to develop an algorithm that is similar to one derived from MAKL, but is based on MAPI approach, in order to expand the horizon of multi-agent algorithms development that is based on probabilistic inference, which is our goal of this thesis.

\subsection{Plausible extensions}
\label{sec:chap1-MEMARL-Extensions}
Now, we have seen the problems regarding the probabilistic inference algorithm. In this section, we are going to provides the reason \textit{why} the problem should be solved. We will argued that the control as probabilistic inference has shown to be very fruitful in re-interpreting some of the reinforcement learning algorithms. In this thesis, we consider 3 extensions to multi-agent reinforcement learning by borrowing single agent concepts and reinterpret as control as probabilistic inference framework that extends to multi-agent setting, which includes: hierarchical reinforcement learning (option based), imitation learning with context awareness and representing/inferring hidden state in partial observable Markov decision process (POMDP) setting. 

\paragraph{Hierarchical Control with Delayed Option Communication:} 
Our motivation starts from the option framework as we want agents to be able to control with temporal abstraction \cite{sutton1999between, bacon2017option}. The setting we are having consists of a master/higher-level agent\footnote{We will use these terms interchangeably} that emits a new option $o_t$ when the termination policy decided to terminate the option before hand $o_{t-1}$ by setting $b_t = 1$. The option will conditions to lower-agent action, which associate with real environment action i.e $\pi(a_t | s_t, o_t)$. We can interpret $o_t$ as the mode of action that the agent can execute. This framework been reinterpreted to probabilistic inference problem in \cite{igl2019multitask} however they lacks closed formed solution which would allow us to apply soft actor-critic (or soft Q-learning) like algorithm. As a minor contribution, we will try to fully solve the probabilistic option framework in single agent case\footnote{There has been an attempts in solving it \cite{lobo2019soft} but there are some mistakes with the derivation as mentioned by the authors. We will point out the error and provide the solution}. After this, we move on to the multi-agent case, which we can fully derived the probabilistic version, which leads, ultimately to, delayed option communication framework proposed in \cite{han2019multi}. 


\paragraph{Imitation learning with Context Awareness: } There has been a significant development in maximum entropy inverse reinforcement learning \cite{ziebart2008maximum} with exciting works on adversarial imitation learning \cite{ho2016generative, fu2017learning, ghasemipour2019divergence} with its connection to generative adversarial network (GAN) \cite{goodfellow2014generative, finn2016connection}. There are also works \cite{petangoda2019disentangled, rakelly2019efficient, yu2019meta} in which the authors tries train the agent to infer the \correctquote{type} of the environment represented in latent variable $z_t$ based on its past experience, so that the generalized policy would act correctly conditioned to the context of the environment $\pi(a_t | s_t, z_t)$ leading to plausible multi-task/meta-learning algorithms. We envisioned the agent to be able to infer others' goals and its value function, which is compressed in single latent variable $z_t^{-i}$\footnote{or in anthropomorphic terms: \textit{personality} (we disagree to use such a term but this might help the reader to understand the motivation behind and the kind of task, we try to solve).} so that it can be robust once encountering opponent that isn't appear in training i.e $\pi(a_t^i | s_t, z^{-i}_t)$. Before, we can reach such model, there are difference algorithms with imitation learning multi-agent reinforcement learning \cite{song2018multi, yu2019multi, liu2020multi} however, all of the models aim to imitate the dataset of trajectories, while our goal is to utilized this framework as an opponent model so that the agent can \textit{benefit}\footnote{This means we want our agent to exploit the knowledge about its opponents that it has and not just imitate} from it. This can be done by exploiting the training procedure, which reduces the problem into single agent problem, while being faithful to control as probabilistic framework. As a minor contribution, We also going to survey multiple single agent imitation learning algorithm as a possible extensions to the multi-agent adversarial framework.

\paragraph{Representing Belief:} In an partial observable setting (either in single agent case or multi-agent case), the agent has to form a belief over the states based on history local observations or information that it received. In single agent case, we normally use recurrent neural network (RNN) as the tools to aggregated these local information and form a belief . However, they are not \correctquote{probabilistic} in the sense that there isn't any uncertainty associated with the state representation, since RNN only acts as a compression machine. There has been developments in representing belief in single agent domain resulting in an algorithm called \textit{Deep variational reinforcement learning} (DVRL) \cite{igl2018deep} that utilized of variational sequential Monte Carlo (VSMC) \cite{le2017auto, maddison2017filtering, naesseth2017variational}, which is an extension of algorithms called sequential Monte Carlo \cite{doucet2001introduction} by parameterized the transition probability and emission probability, so that the agent could jointly learn the model of the environment and maximizing its reward. There has been a variance of DVRL \cite{shvechikovjoint}, which provides some insight into how similar algorithms can be derived via control as probabilistic inference framework\footnote{We are aware of the work done in \cite{huangsvqn} however they didn't employ any importance weighting or particle filter that would make the variational bound tighter (and a representation of uncertainty), which is the main aim of DVRL and ours, while \cite{shvechikovjoint} results resemblance \cite{igl2018deep} much closer. We will discuss the differences in section in chapter \ref{chapter:chap9}}. In a nutshell, VSMC maintains a set of particles, where each of them is a tuple of 2 elements: state representation $s_t$ and its associated weight $w_t$\footnote{We have simplify the description but the arguments still holds. In DVRL, the authors assign particle as a 3 elements tuple: $(z_t, h_t, w_t)$ where $z_t$ is latent state representation, $h_t$ is the hidden representation of RNN, and $w_t$ is the weight of each particle, which acts as uncertainty quantifier.}, which is updated every time the agent acts given its local information. There has been works done in employing sequential Monte Carlo method in multi-agent case \cite{doshi2009monte} via Interactive-Partial observable Markov decision process (I-POMDP) formulation \cite{gmytrasiewicz2005framework}, and recently, via the extension of value iteration network \cite{tamar2016value} that allows belief update on POMDP \cite{karkus2017qmdp}, The work in \cite{han2019ipomdp} further extends POMDP framework to works in multi-agent case of I-POMDP. The biggest problem with I-POMDP is that modeling nested belief can be computationally expensive. Given a cooperative setting, we can, alternatively, consider a public belief MDP framework proposed in \cite{nayyar2013decentralized}, which has been successful scaling up via approximation and neural network in \cite{foerster2018bayesian, hu2019simplified}. The public belief MDP reduces multi-agent problem into POMDP problem and remove nested belief consideration in I-POMDP, which carves the way for us to apply control as probabilistic inference framework to multi-agent problem. 



