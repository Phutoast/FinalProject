\section{Motivation}

\subsection{Incompatibility within MEMARL framework}

In MEMARL family of algorithms, we can divide algorithms into 2 main types: multi-agent learning with probabilistic inference (MAPI) and multi-agent learning with KL-constraint (MAKL). Both types are difference in term of how they arrived at the final algorithm. Notes that KL-divergence with uniform algorithms is the same as entropy (positive or negative depends on the side 2 probability are on). Both of them are algorithms that mainly maximizes its reward with some KL-constrain so that the final policy is some what near the prior, which is directly inspired by control-as-inference single agent framework and bounded rationality. However, there are subtle incompatibility in the way both kind of algorithms are developed. 

We will use ROMMEO \cite{tian2019regularized} as a candidate algorithm that is derived from MAPI perspective, others includes PR2 \cite{wen2019probabilistic} and GR2 \cite{wen2019multi}. MAPI initial goal for development is to facilitate recursive reasoning within agent by representing agent and its opponent to be conditional probability of others action, i.e in ROMMEO case the joint probability is factorized as
\begin{equation*}
    \pi(a^{i}, a^{-i} | s) = \pi(a^{i} | s, a^{-i}) \rho(a^{-i} | s)
\end{equation*}
where $a^i$ is the agent's action and $a^{-i}$ is the action of other agents. The main differences for these framework are how factorization is made. The authors introduces an optimality random variable of agent $i$ (which is now defined only for cooperative game) $\mathcal{O}^{i}_t$ that indicates whether its acts optimal or not at time $t$, and is defined proportional to exponential of joint reward.
\begin{equation*}
    P(\mathcal{O}^{i}_t = 1 | \mathcal{O}^{-i}_t = 1) \propto \exp\left( \beta r(s_t, a^i_t, a^{-i}_t) \right)
\end{equation*}
where $\beta$ is called temperature variable. Note that defining an optimality random variable is a common technique in a super-set of control-as-inference framework, as we want to maximizing the probability of this throughout the whole time step. This can be achieved using variational inference. This leads agent's objective to be maximizing the reward with the constraint controlled by $\beta$, so that the final policy is near to the prior. The opponent model's objective is to maximize the reward with similar KL-constrain as agent's policy. The beauty lies in the fact that the agent can also optimizes its opponent model so that it is "optimal" rather than just represent other player's policy which partially solve the problem of centralized training. However, this also posts a \textit{huge} problem, since in zero-sum case the agent will optimizes its model so that its acts as if the opponent is maximizing its own reward, which is completely opposite behavior to what we desire.

For MAKL algorithm, the most prominent algorithm is Balancing-Q learning \cite{grau2018balancing}, in which the authors draws an inspiration from KL-constrained single agent learning, which leads to following loss 
\begin{equation*}
    \mathbb{E}\left[ \sum^T_{t=0} \gamma^t \left(R(s_t, a_t, a^{-i}_t) - \frac{1}{\beta^i} \log\frac{\pi(a_t|s_t)}{P_{\operatorname{prior}}(a_t|s_t)} - \frac{1}{\beta^{-i}}\log \frac{\rho(a^{-i}_t|s_t)}{P_{\operatorname{prior}}(a^{-i}_t|s_t)} \right) \right]
\end{equation*}
This is seen as putting difference constrains $\beta^i, \beta^{-i}$ on both agents and opponents, respectively, in which. The the case where $\beta^{-i}$ is negative and $\beta^i$ is positive the resulting algorithms are acting in zero-sum manner. And, if both of them are positive, then the game becomes cooperative. This is very interesting, since this essentially filled the gap within in MAPI framework. However, there isn't any obvious way to further extend this algorithms or the way to map this approach to MAPI framework.

In summary, MAPI provided us with principled approach to develop multi-agent reinforcement algorithms, however, it lacks expressiveness to solve zero-sum game. On the other hand, MAKL provided us with baked formulation that allows us to works on zero-sum game, which can be a difficulty when we want to extends the current MEMARL framework. We will need to find a way so to develop an algorithm that is similar to one derived from MAKL, but is based on MAPI approach. For a more details on all algorithms including interpretation of temperature variable $\beta$ and implementations, please refer to \hyperref[chapter:chap1]{background chapter}.

\Phu{Adding Inverse-RL problem + have to restucture abit so that it shows importance of this}

\subsection{Plausible extensions}
Control-as-inference framework has been very fruitful on interpreting some reinforcement learning algorithms techniques, such as hierarchical control, inferring hidden state within partial observable markov decision process (POMDP), and information theoretic extensions, mainly empowerment for exploration etc. as probabilistic inference problem. In this section, we will briefly describe on how both additional features can be interpreted and be merged into multi-agent problem.

Start of with hierarchical control, which we will based on algorithm MSOL \cite{igl2019multitask}, presents an base agent $\pi(a_t | s_t, z_t)$ coupled with higher-level agent $\pi^H(z_t |s_t, z_{t-1}, b_t)$ and terminal agent $\pi^T(b_t | s_t, z_{t-1})$. The base-agent will receives an instruction from higher-level agent to acts on certain behavior defined by $z_t$, and the higher-level agent can switch base-agent behavior, when the terminal agent sends a message to change $b_t$ (or to terminal the current behavior). Given this formulation, the agent is able reason in extended temporal setting, while being able to perform multi-task learning, which speeds up learning. This will be very important extension to multi-agent reinforcement learning. Furthermore, by having a higher-agent manipulating base agent by generating $z_t$ is similar to MAVEN \cite{mahajan2019maven}, a new state-of-the-art multi-agent reinforcement learning algorithm, which is shown empirically to helps agent explore by letting it perform difference modes of sequence of action. 

For the more fundamental question of how to represents belief, we turn to a problem of POMDP. Normally, one would use recurrent neural network, such as LSTM \cite{hochreiter1997long} or GRU \cite{cho2014properties} to keep track of history of observations, and compute accumulated information to act. However, this approach lacks representation of epistemic uncertainty in the current state of the world. One promising approach is to quantify uncertainty via particle filtering \cite{igl2018deep} where each particle is a 3-element tuple of $(z_t, h_t, w_t)$, where $z_t$ is latent state representation (since we don't know the actual state), $h_t$ is the hidden representation of RNN, and $w_t$ is the weight of each particle, which acts as uncertainty quatifier. Given the particle, the agent can use them to arrived at the final action $a_t$. We can interpret the weight as uncertainty quantifier. The current work \cite{shvechikovjoint}  has interpret this approach as a probabilistic inference with sequential monte carlo sampling. Given the tools developed so far, there are 2 main extensions to multi-agent algorithms that we can do: POMDP-MARL, quantify uncertainty of other agents behavior (via "ensemble" of opponent models) and inferring belief of other agents (theory of mind). For all of the extensions, the agents are required to have the understanding of the observation other agents receive. This problem can be framed as multi-viewed problem \cite{li2019multi}, which we can employ developed generative models, such as generative query network \cite{eslami2018neural}. 

Finally, we employ the current development of information-theoretic based single-agent reinforcement learning problem, which has been shown to be a more generalization of most of control-as-inference framework \cite{grau2018soft, leibfried2019unified}. This framework is based on the ability to optimizes prior term, leading to following optimization problem (in a brief form)
\begin{equation*}
    \max_{\pi, \rho} \mathbb{E} \Bigg[ \sum^\infty_{i=0} \gamma^{t} \bigg( r(s_t, a_t) - \frac{1}{\beta}\frac{\pi(a_t|s_t)}{\rho(a_t)} \bigg) \Bigg]
\end{equation*}
The author has shown that by optimizing prior this way, one recover reward maximization algorithm with mutual-information (MI) constraint, and the author has shown that this framework can promote exploration. In term of extension to multi-agent reinforcement learning algorithm, we can have better prior over the opponent model with better exploration. 