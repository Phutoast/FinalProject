\label{sec:chap1-Outline}
This outline in this thesis is as follow list. Chapter \ref{chapter:chap3} onward are the extensions we make given our understanding of MEMARL framework explored in chapter \ref{chapter:chap3}.
\begin{itemize}
    \item \textbf{\hyperref[chapter:chap2]{Chapter 2} (Background)}: We will go through the tools necessary to develop the algorithms and formulation of the problem in multi-agent reinforcement learning (MARL)\footnote{We will include an introduction to chapter specific algorithm (for example the explanation of Baysian Action Decoder \cite{foerster2018bayesian} and DVRL as probabilistic graphical model \cite{shvechikovjoint} are in chapter \ref{chapter:chap5} ) to that chapter. This would reduce the space of the background and make the information easy to search. The intention of this chapter is to the readers to understand the current landscape of MARL algorithms.}, which includes 
    \begin{itemize}
        \item Variational inference techniques, Expectation Maximization (EM), and Stein Variational Gradient Descent (SVGD) \cite{liu2016stein}, which will be used in developing and implementing most of the algorithms.
        \item Single agent reinforcement learning: MDP and POMDP formulation, value iteration, and its contraction proof + improvement theorem\footnote{We would like to restate those proof because we will encounter similar proof throughout the thesis}
        \begin{itemize}
            \item Basic Deep reinforcement learning, including
            \begin{itemize}
                \item  Deep Q-learning \cite{mnih2015human} (and their variances such as double Q-learning \cite{van2016deep}).
                \item Policy Gradient \cite{sutton2000policy} (and their variance such as advantage actor critic (A2C) \cite{mnih2016asynchronous}\footnote{The citation mainly develop asynchronous update hence the name Asynchronous A2C or A3C. The synchronous version is presented in \url{https://openai.com/blog/baselines-acktr-a2c/}}, Trust Region Policy Optimization (TRPO) \cite{schulman2015trust} and Proximal Policy Optimization (PPO) \cite{schulman2017proximal}).
                \item Deep Deterministic Policy Gradient (DDPG) \cite{lillicrap2015continuous} and Twin Delayed DDPG (TD3) \cite{fujimoto2018addressing}.
            \end{itemize}
            \item Control as inferences frameworks, soft Q-learning \cite{haarnoja2017reinforcement}, and soft Actor-critic \cite{haarnoja2018softa, haarnoja2018softb}
            \item We will, also, includes recent works on bounded rationality as an alternative explanation of reinforcement learning with KL-divergence constraint \cite{ortega2013thermodynamics, tim2015bounded}.
        \end{itemize}
        % \item Extensions to control as inference frameworks: Hierarchical reinforcement learning \cite{igl2019multitask}, representing belief in POMDP \cite{igl2018deep}, and inverse-reinforcement learning \cite{fu2017learning}. 
        \item Multi-agent reinforcement learning (MARL) algorithms: 
        \begin{itemize}
            % \item We will go through DEC-POMDP \cite{bernstein2002complexity} formulation of MARL problem and other related MDPs for multi-agent. This would include the notion of stochastic game and Nash Equilibrium.
            \item We will go through stochastic game \cite{shapley1953stochastic}. This would include the notion of Best Response and Nash Equilibrium \cite{nash1950equilibrium}. 
            \item We also going to give overviews of value learning based multi-agent reinforcement learning algorithms, which is related to some of the algorithms in the latter chapters. This includes: Nash Q-learning \cite{hu2003nash} and Friend-or-Foe Q-learning \cite{littman2001friend}.
            \item we will survey importance deep multi-agent reinforcement learning algorithms, including: Multi-Agent DDPG \cite{lowe2017multi}, Counterfactual Multi-Agent Policy Gradients (COMA-PG) \cite{foerster2018counterfactual}, Value Decomposition Network \cite{sunehag2017value}, QMIX \cite{rashid2018qmix}, and Multi-Agent Variational Exploration (MAVEN) \cite{mahajan2019maven}. 
        \end{itemize}
    \end{itemize}
    \item \textbf{\hyperref[chapter:chap3]{Chapter 3} (Unified view)}: In this chapter, we will consider the full survey of MEMARL framework and try to bring every algorithm into single control as probabilistic inference framework, which is done by examining how each algorithm is derived and observe the standard pattern. This would resulted in a probabilistic version of Balancing Q-learning, which is based on probabilistic inference framework.
    % \item \textbf{\hyperref[chapter:chap4]{Chapter 4} (General Sum Extension)}: Given the probabilistic formulation of Balancing Q-learning learning, we would like to provide an simple extensions to accommodate general sum setting, extending the capacity of Balancing Q-learning.
    % \item \textbf{\hyperref[chapter:chap5]{Chapter 5} (EM-Based MARL)}: Now, we shall apply multi-agent problem to EM-Based algorithm for reinforcement learning, which is based, mainly, on VIREL and MPO. This would lead to an interesting algorithm that has been examined in other setting. 
    \item \textbf{\hyperref[chapter:chap4]{Chapter 4} (Hierarchy and Delayed communication MARL)}:By this chapter, we have examined, thoroughly, control as probabilistic inference framework's application on multi-agent setting. We will turn toward augmenting agent to be able to reason in temporal abstraction, which leads to delayed option communication framework. As a minor contribution, we also provide a soft-option learning algorithm for single-agent reinforcement learning based on soft Actor-Critic algorithms.
    % \item \textbf{\hyperref[chapter:chap7]{Chapter 7} (Imitation Learning in MARL)}: This chapter will be in slightly difference theme compare to the others. That is because, we are dealing with imitation learning of the opponents and opponent context inference so that our agent cound exploit its knowledge about the opponent. We will so surveying plausible extension to adversarial imitation learning to newly extended framework we proposed. 
    % \item \textbf{\hyperref[chapter:chap8]{Chapter 8} (Revisiting Generalized Recursive Reasoning)}: We will turn our attention to recursive reasoning in multi-agent setting. We investigate the current generalized recursive reasoning framework \cite{wen2019multi}, we point out some issues that requires extra attention (notably how the action value function is represented). In turn, we will try to proposed an alternative training of generalized recursive reasoning, which should be more faithful toware control as probabilistic framework. This would require some observation given in chapter \ref{chapter:chap3}. 
    \item \textbf{\hyperref[chapter:chap5]{Chapter 5} (Reinterpretation Public Belief MDP)}: Finally, we will propose a re-interpretation of public belief MDP via control as inference framework. This would lead to VSMC like algorithm, which allows the agents to reason in a counterfactual manner without the need for recursive reasoning.
    \item \textbf{\hyperref[chapter:chap6]{Chapter 6} (Conclusion and Future Work)}: We will reviews our progress so far and conclude with future works and research direction. 
\end{itemize}
In the appendixes, we will provide a full derivation of some of the algorithms. We believe that some of the techniques can be useful, thus worth to stated in a complete form. Appendixes will be divided by the content of each chapter. 