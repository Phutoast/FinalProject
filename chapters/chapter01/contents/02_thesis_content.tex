In chapter \ref{chapter:chap3}, we tackles the MAMERL problem, where we showed that all of the algorithms in the MAPI framework doesn't work on the competitive setting. Given the algorithm called Balancing Q-learning, which can work on both cooperative and competitive setting, we reinterpret it via the lens of MAPI framework. This has resulted in probabilistic Balancing Q-learning, which is an algorithm that is similar to Balancing Q-learning while being derived from MAPI framework. By constructing such an algorithm, we have unified MAMERL framework, so that all the algorithms in this framework are based on variational inference technique. 

In chapter \ref{chapter:chap4}, we solve single-agent soft-option learning problem by finding a solution of the objective, initially, proposed in \cite{igl2019multitask}, which leads us to the development of soft actor-critic like, training algorithm, instead of solving the problem via policy gradient. We move on to define an objective for delayed communication and options framework in a multi-agent setting. We show the full derivation of the solution that would optimize the objective we have described. The final optimal answer is, then, turned into an algorithm that utilizes soft Actor-Critic like algorithm allowing off-policy training procedure. 

Finally, in chapter \ref{chapter:chap5}, we aimed to extend the current MAPI algorithms so that the agents have the belief on the hidden state. Naturally, this would require us to perform a nested belief update. However, via public-belief MDP, we reduce the problem into a POMDP problem, where we can apply control as inference framework. We constructed two components, which are public agent, that performs a belief update, and a low-level agent, that acts on its local information and instruction from the public agent. Finally, we conclude this chapter with the implementation of both components. 