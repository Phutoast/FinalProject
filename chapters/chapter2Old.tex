We will start by formalizing difference graphical models representing our assumptions on agents and their interaction. This will lead to ELBO representing methods in difference literature including:  \cite{tian2019regularized}, \cite{grau2018balancing}, and \cite{wen2019probabilistic}. Furthermore, we will extend this framework to new objective function. Let's start with one-step game, and extending to stochastic game can be shown to be arbitrary.

\section{Single-Step ROMMEO \cite{tian2019regularized}}

This is based on the perspective of agent $i$.

\begin{figure}[ht]
    \begin{minipage}[t]{0.5\linewidth}
    \centering
    \begin{tikzpicture}[latent/.append style={minimum size=1.0cm}]
        \node[obs] (o) {$\mathcal{O}^i$};
        \node[latent, below=of o] (a) {$\boldsymbol{a}^i$};
        \node[latent, below=of a] (s) {$\boldsymbol{s}$};
        \node[latent, below=of s] (a1) {$\boldsymbol{a}^{-i}$};
        \node[obs, below=of a1] (o1) {$\mathcal{O}^{-i}$};
        
        
        \path[->]  (o1)  edge   [bend left=45] node {} (a);
        \edge{o1} {a1}
        \edge {s} {a, a1}
        \edge {a} {o}
        \path[->]  (o1)  edge   [bend left=60] node {} (o);
        \path[->]  (a1)  edge   [bend left=45] node {} (o);
        \path[->]  (a1)  edge   [bend left=30] node {} (a);
        \path[->]  (s)  edge   [bend left=30] node {} (o);
    \end{tikzpicture}
    \label{ROMMEO-Graphical}
    \end{minipage}%
    \begin{minipage}[t]{0.5\linewidth}
    Graphical model of One-Step ROMMEO \cite{tian2019regularized}. We can see that the optimality of the agent $\mathcal{O}^{i}$ is based on the optionally of other agents $\mathcal{O}^{-i}$. Furthermore, the action of opponent agents is assumed to be optimal.
    \end{minipage}
\end{figure}

% \begin{figure}[!h]
 
% \end{figure}
Assuming the opponent model is \emph{Optimal} i.e $P(\mathcal{O}^{-i} = 1) = 1$ As we would like to find the variational distribution $q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{-i} = 1)$, which can be represented (factorized) as 

\begin{equation}
    q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{-i} = 1) = \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}) \cdot \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) \cdot P(\boldsymbol{s})
\end{equation}
This implies that ELBO can be found by minimizing the KL-Divergence between $q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{-i} = 1)$ and $P(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{i} = \mathcal{O}^{-i} = 1)$, where (The joint distribution factorization following \ref{ROMMEO-Graphical})

\begin{equation}
    \begin{aligned}
        P(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{a}^{-i} | \mathcal{O}^{i} = \mathcal{O}^{-i} = 1) &= \frac{P(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}, \mathcal{O}^{i} = 1,  \mathcal{O}^{-i} = 1)}{\int P(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}, \mathcal{O}^{i} = 1,  \mathcal{O}^{-i} = 1) \ d\boldsymbol{s} \ d\boldsymbol{a}^i \ d\boldsymbol{a}^{-i}} \\ 
        &\propto \begin{aligned}[t]
            &P(\mathcal{O}^i = 1 | \boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) \cdot P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) \cdot \\
            &P(\boldsymbol{s}) \cdot P(\boldsymbol{a}^{-i} |\boldsymbol{s}, \mathcal{O}^{-i} = 1) \cdot P(\mathcal{O}^{i})
        \end{aligned}
    \end{aligned}
\end{equation}

In \cite{tian2019regularized}, the game is assumed to be fully cooperative game, meaning that we can define the optimality of agent $it$ to be (with respect to opponent being optimal)

\begin{equation}
    P(\mathcal{O}^i = 1 | \boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) \propto \exp \left( \beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \right)
\end{equation}
By finding the KL-Divergence between variational distribution and posterior, we have 

\begin{equation}
    \begin{aligned}
        \mathbb{E}_{(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \sim q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{-i} = 1)} &\Big[\log q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{-i} = 1) - \log P(\boldsymbol{s}, \boldsymbol{a}, \boldsymbol{a}^{-i} | \mathcal{O}^{i} = \mathcal{O}^{-i} = 1) \Big] \\
        &= \begin{aligned}[t]
                \mathbb{E}\Big[ \log \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}) &+ \log \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) + \cancel{\log P(\boldsymbol{s})} \\
                &-\log P(\mathcal{O}^i = 1 | \boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) \\
                &- \log P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) - \cancel{\log P(\boldsymbol{s})} \\
                &- \log P(\boldsymbol{a}^{-i} |\boldsymbol{s}, \mathcal{O}^{-i} = 1) - \log P(\mathcal{O}^{i}) \Big] 
            \end{aligned} \\
        &= \begin{aligned}[t]
                &\mathbb{E}\Big[ \log \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}) -  \log P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) \Big] \\ 
                &+\mathbb{E}\Big[ \log \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) - \log P(\boldsymbol{a}^{-i} |\boldsymbol{s}, \mathcal{O}^{-i} = 1) \Big] \\
                &-\beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) -\log 1
            \end{aligned}
    \end{aligned} 
\end{equation}
This is equal to maximizing the following objective as defined in \cite{tian2019regularized}, with single step (Where the authors defined $P_{\text{prior}}$ to be uniform distribution over actions)
\begin{equation}
    \begin{aligned}
        \mathbb{E}_{(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \sim q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{-i} = 1)}\bigg[R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) &- \frac{1}{\beta} D_{KL} \Big(\pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}) \Big|\Big| P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) \Big)  \\
        & - \frac{1}{\beta} D_{KL} \Big( \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) \Big|\Big| P(\boldsymbol{a}^{-i} |\boldsymbol{s}, \mathcal{O}^{-i} = 1) \Big)\bigg]
    \end{aligned}
\end{equation}
From this objective, we can now derived the closed form solution of $\pi_{\theta}$ and $\rho_{\phi}$. This would be a foundation for extending this framework into stochastic game. The derivation of $\pi_{\theta}$ and $\rho_{\phi}$ follows from \cite{levine2018reinforcement}, which we make it more explicit for multi-agent case. 
\noindent
Starting with defining the normalizing term, in which is a constant with respect to $\boldsymbol{a}^i$, calling it $N(\boldsymbol{s}, \boldsymbol{a}^{-i})$, where its value will be clear after the derivation.

\begin{equation}
    \begin{aligned}
        &\begin{aligned}[t]
            \mathbb{E}_{(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \sim q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{-i} = 1)} &\Big[ \beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) - \log \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}) + \log  P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) \\
            & - \log \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) + \log P(\boldsymbol{a}^{-i} |\boldsymbol{s}, \mathcal{O}^{-i} = 1) \\
            &+ N(\boldsymbol{s}, \boldsymbol{a}^{-i}) - N(\boldsymbol{s}, \boldsymbol{a}^{-i}) \Big]
        \end{aligned} \\
        &= \begin{aligned}[t]
            &\mathbb{E}_{(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \sim q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{-i} = 1)} [N(\boldsymbol{s}, \boldsymbol{a}^{-i})] \\
            &-\begin{aligned}[t]
                \mathbb{E}_{(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \sim q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{-i} = 1)} \Big[&-\log \exp\left( \beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})\right) + \log \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}) \\
                &-\log P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) + \log \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) \\
                &-\log P(\boldsymbol{a}^{-i} |\boldsymbol{s}, \mathcal{O}^{-i} = 1)  + \log \exp \left(N(\boldsymbol{s}, \boldsymbol{a}^{-i})\right)\Big]
            \end{aligned}   
        \end{aligned} \\
        &= \begin{aligned}[t]
            &\mathbb{E}_{(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \sim q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{-i} = 1)} [N(\boldsymbol{s}, \boldsymbol{a}^{-i})] \\
            &\begin{aligned}[t]
                -\mathbb{E}_{(\boldsymbol{s}, \boldsymbol{a}^{-i}) \sim q(\boldsymbol{s}^i, \boldsymbol{a}^{-i})}\Big[ \mathbb{E}_{a^i \sim \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{a}^{-i}, \boldsymbol{s})}&\Big[ \log \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{a}^{-i}, \boldsymbol{s}) - \Big[\log\exp\left(\beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})\right) \\
                &+ \log P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) - \log \exp \left(N(\boldsymbol{s}, \boldsymbol{a}^{-i})\right)\Big]\Big] \\
                &+ \log \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) - P(\boldsymbol{a}^{-i} |\boldsymbol{s}, \mathcal{O}^{-i} = 1)\Big]
            \end{aligned}
        \end{aligned} \\
        &= \begin{aligned}[t]
                &\mathbb{E}_{(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \sim q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{-i} = 1)} [N(\boldsymbol{s}, \boldsymbol{a}^{-i})] \\
                &\begin{aligned}[t]
                    -\mathbb{E}_{(\boldsymbol{s}, \boldsymbol{a}^{-i}) \sim q(\boldsymbol{s}^i, \boldsymbol{a}^{-i})}\bigg[ &D_{KL}\left( \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}) \bigg|\bigg| \frac{\exp (\beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}))}{\exp( N(\boldsymbol{s}, \boldsymbol{a}^{-i}))} P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) \right) \\
                    &+ \log \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) - P(\boldsymbol{a}^{-i} |\boldsymbol{s}, \mathcal{O}^{-i} = 1)\bigg]
                \end{aligned}
            \end{aligned} \\
    \end{aligned} 
\end{equation}
We can see that if we want to maximize this with respect to $\pi_{\theta}$, we only have to minimizes KL-Divergence, which we have to set 
\begin{equation}
    \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}) = \frac{\exp (\beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})) P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1)}{\exp( N(\boldsymbol{s}, \boldsymbol{a}^{-i}))}
\end{equation}
The normalizing factor is trivial to find, which is equal to 
\begin{equation}
    N(\boldsymbol{s}, \boldsymbol{a}^{-i}) = \log \int_{\mathcal{A}^i} \exp (\beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})) P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) \ da^{i}
\end{equation}
For the optimal opponent model, we first find  $\pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}) \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1)$ using the same technique/loss 

\begin{equation}
    \begin{aligned}
        &\begin{aligned}[t]
            &\mathbb{E}_{(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \sim q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{-i} = 1)} [N(\boldsymbol{s})] \\
            &-\begin{aligned}[t]
                \mathbb{E}_{(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \sim q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{-i} = 1)} \Big[&-\log \exp\left( \beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})\right) + \log \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}) \\
                &-\log P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) + \log \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) \\
                &-\log P(\boldsymbol{a}^{-i} |\boldsymbol{s}, \mathcal{O}^{-i} = 1)  + \log \exp \left(N(\boldsymbol{s})\right)\Big]
            \end{aligned}   
        \end{aligned} \\
        &= \begin{aligned}[t]
            \mathbb{E}_{(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \sim q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{-i} = 1)} \Big[ \mathbb{E}_{(\boldsymbol{a}^i, \boldsymbol{a}^{-i}) \sim q(\boldsymbol{a}^i, \boldsymbol{a}^{-i})} &\Big[ \log \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}) + \log \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) \\
            - &\Big[ \log\exp\left(\beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})\right) + \log P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1) \\
            & - \log \exp \left(N(\boldsymbol{s})\right) + \log P(\boldsymbol{a}^{-i} |\boldsymbol{s}, \mathcal{O}^{-i} = 1) \Big]\Big] \Big]
        \end{aligned} \\
        &= \begin{aligned}[t]
            \mathbb{E}_{(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \sim q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i} | \mathcal{O}^{-i} = 1)} \Bigg[ D_{KL}\Bigg( &\pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}) \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) \Bigg|\Bigg| \\
            &\frac{\exp\left(\beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})\right)P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1)P(\boldsymbol{a}^{-i} |\boldsymbol{s}, \mathcal{O}^{-i} = 1)}{\exp (N(\boldsymbol{s}))} \Bigg)\Bigg]
        \end{aligned}
    \end{aligned}
\end{equation}
We can recover the normalizing factor $N(\boldsymbol{s})$ to be equal to 
\begin{equation}
    \log \int_{\mathcal{A}^i} \int_{\mathcal{A}^{-i}} \exp\left(\beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})\right)P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1)P(\boldsymbol{a}^{-i} |\boldsymbol{s}, \mathcal{O}^{-i} = 1) \  d\boldsymbol{a}^{-i} \ d\boldsymbol{a}^i
\end{equation}
Hence, we have the joint probability distribution to be equal to 
\begin{equation}
    \pi_{\theta}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}) \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) = \frac{\exp\left(\beta R(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})\right)P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \boldsymbol{a}^{-i}, \mathcal{O}^{-i} = 1)P(\boldsymbol{a}^{-i} |\boldsymbol{s}, \mathcal{O}^{-i} = 1)}{\exp (N(\boldsymbol{s}))}
\end{equation}
Substitute agent's action, we can recover optimal opponent model, which is 
\begin{equation}
    \rho_{\phi}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \mathcal{O}^{-i} = 1) = \frac{\exp (N(\boldsymbol{s}, \boldsymbol{a}^{-i})) P(\boldsymbol{a}^{-i} |\boldsymbol{s}, \mathcal{O}^{-i} = 1)}{\exp (N(\boldsymbol{s}))}
\end{equation}
The extension to stochastic game version can be found in appendix \ref{ROMMEOFull}. This will include derivation of $Q$-function via message passing. 

\emph{Note:} In the original work, \cite{tian2019regularized} defined a weighting parameter $\alpha$ on KL Term between agent's policy and agent's prior (in which the authors assume a uniform prior). This doesn't directly correspond to our framework developed, since the rationality and intention ($\beta$) of both agent must be the same in ROMMEO's case. This can be seen as ROMMEO's restriction on cooperative game.
 
\section{Balance Soft Q-Learning \cite{grau2018balancing}}
In this work, the authors use $\beta$ variables to control the semantic of the game and agents' rationality. The game itself is based on centralized reward $\mathcal{R}_G$.  
\begin{figure}[ht]
    \begin{minipage}[t]{0.5\linewidth}
    \centering
    \begin{tikzpicture}[latent/.append style={minimum size=1.0cm}]
        \node[obs] (o) {$\mathcal{R}_G$};
        \node[latent, below=of o] (a) {$\boldsymbol{a}^i$};
        \node[latent, below=of a] (s) {$\boldsymbol{s}$};
        \node[latent, below=of s] (a1) {$\boldsymbol{a}^{-i}$};
        
        \node [obs, above right=1.3cm of a] (beta)  {$\beta_i$};
        
        \node [obs, above right=1.3cm of a1] (beta1)  {$\beta_{-i}$};
    
        \edge {s} {a, a1}
        \edge {beta1} {a1}
        \edge {beta} {a}
        \edge {a} {o}
        \path[->]  (a1)  edge   [bend left=45] node {} (o);
        \path[->]  (s)  edge   [bend left=30] node {} (o);
    \end{tikzpicture}
    \label{Balancing-Graphical}
    \end{minipage}%
    \begin{minipage}[t]{0.5\linewidth}
    Graphical model of One-Step of the algorithm defined in  \cite{grau2018balancing}. We can see that in this work, authors doesn't assume optimality of opponent model. Instead, they assume that the action of the opponent is known. Further more, the $\beta$-value is also known to agents and their opponent.
    \end{minipage}
\end{figure}
We can define the variational distribution to be equal to 
\begin{equation}
     q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}, \beta_i = \beta_{\text{pl}}, \beta_{-i} = \beta_{\text{op}}) = P(\boldsymbol{s}) \cdot P(\boldsymbol{a}^i | \boldsymbol{s}, \beta_i = \beta_{\text{pl}}) \cdot P(\boldsymbol{a}^{-i} | \boldsymbol{s}, \beta_i = \beta_{\text{op}})
\end{equation}
In which, we define the agent's policy and opponent's policy to be in the form of
\begin{equation}\label{balance-pol-op}
     P(\boldsymbol{a}^i | \boldsymbol{s}, \beta_i = \beta_{\text{pl}}) = \frac{\pi_{\theta}(\boldsymbol{a}^i |\boldsymbol{s})^{\frac{1}{\beta_{\text{pl}}}}}{N_{\text{pl}}(\boldsymbol{s})} = \frac{e^{\frac{1}{\beta}_{\text{pl}} \log \pi_{\theta}(\boldsymbol{a}^i | s)} }{N_{\text{pl}}(s)} \quad P(\boldsymbol{a}^{-i} | \boldsymbol{s}, \beta_i = \beta_{\text{pl}}) = \frac{\rho_{\phi}(\boldsymbol{a}^{-i} |\boldsymbol{s})^{\frac{1}{\beta_{\text{op}}}}}{N_{\text{op}}(\boldsymbol{s})}
\end{equation}
Similarly, the joint probability distribution that we want to estimate is defined to be 
\begin{equation}
    \begin{aligned}
        P(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}, \mathcal{O} = 1, \beta_i = \beta_{\text{pl}}, \beta_{-i} = \beta_{\text{op}}) = &P(\mathcal{R}_G | \boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})\cdot P(\bold{s}) \cdot \\
        &P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \beta_i = \beta_{\text{pl}}) \cdot P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \beta_i = \beta_{\text{opp}}) \cdot \\
        &P(\beta_i) \cdot P(\beta_{-i})
    \end{aligned}
\end{equation}
In which, the priors for agent and opponent is defined to be (slightly abusing a notation)
\begin{equation}
    P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}, \beta_i = \beta_{\text{pl}}) = \frac{P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})^{\frac{1}{\beta_{\text{pl}}}}}{M_{\text{pl}}(\boldsymbol{s})} \quad \quad P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s}, \beta_i = \beta_{\text{op}}) = \frac{P_{\text{prior}}(\boldsymbol{a}^{-i} | \boldsymbol{s})^{\frac{1}{\beta_{\text{op}}}}}{M_{\text{op}}(\boldsymbol{s})}
\end{equation}
The normalizing factor is defined in the form of 
\begin{equation}
    \int P_{\xi}(x | y)^{\alpha} \ dy \quad \text{ or } \quad \int \exp\left(\frac{1}{\alpha} \log P_{\xi}(y)\right) \ dy 
\end{equation}
The game reward for agents is defined to be 
\begin{equation}
    P(\mathcal{R}_G | \boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) = \exp \left( \mathcal{R}_{G}(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \right)
\end{equation}
Doing the variational approximation, we have that 
\begin{equation}
    \begin{aligned}
        &\begin{aligned}[t]
            \mathbb{E}_{(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) \sim q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}, \beta_i = \beta_{\text{pl}}, \beta_{-i} = \beta_{\text{op}})}\Big[ &\log q(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}, \beta_i = \beta_{\text{pl}}, \beta_{-i} = \beta_{\text{op}}) \\
            &- \log P(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}, \mathcal{O} = 1, \beta_i = \beta_{\text{pl}}, \beta_{-i} = \beta_{\text{op}}) \Big]
        \end{aligned} \\
        &= \begin{aligned}[t]
            \mathbb{E}\Bigg[\cancel{\log P(\boldsymbol{s})} &+\log \left[\frac{\pi_{\theta}(\boldsymbol{a}^i |\boldsymbol{s})^{\frac{1}{\beta_{\text{pl}}}}}{N_{\text{pl}}(\boldsymbol{s})}\right] + \log \left[ \frac{\rho_{\phi}(\boldsymbol{a}^{-i} |\boldsymbol{s})^{\frac{1}{\beta_{\text{op}}}}}{N_{\text{op}}(\boldsymbol{s})} \right] \\
            &- \log\left[\frac{P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})^{\frac{1}{\beta_{\text{pl}}}}}{M_{\text{pl}}(\boldsymbol{s})}\right] -\log\left[ \frac{P_{\text{prior}}(\boldsymbol{a}^{-i} |\boldsymbol{s})^{\frac{1}{\beta_{\text{op}}}}}{M_{\text{op}}(\boldsymbol{s})}\right] \\
            &- \log P(\mathcal{R}_G | \boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})-\cancel{\log P(\boldsymbol{s})}  \\
            &- \cancelto{0}{\log P(\beta_i)} - \cancelto{0}{\log P(\beta_{-i})}\Bigg] 
        \end{aligned}  \\
        &=\begin{aligned}[t]
            \mathbb{E}\Bigg[\frac{1}{\beta_{\text{pl}}}&\log \pi_{\theta}(\boldsymbol{a}^i |\boldsymbol{s}) - \log N_{\text{pl}}(\boldsymbol{s}) + \frac{1}{\beta_{\text{op}}}\rho_{\phi}(\boldsymbol{a}^{-i} |\boldsymbol{s}) - \log N_{\text{op}}(\boldsymbol{s}) \\
            &- \frac{1}{\beta_{\text{pl}}}\log P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s}) + \log M_{\text{pl}}(\boldsymbol{s})  -\log \frac{1}{\beta_{\text{op}}}P_{\text{prior}}(\boldsymbol{a}^{-i} |\boldsymbol{s}) + \log M_{\text{op}}(\boldsymbol{s}) - \mathcal{R}_{G}(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})\Bigg] 
        \end{aligned} \\
        &=\begin{aligned}[t]
            \mathbb{E}\Bigg[ - \mathcal{R}_{G}(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) + \frac{1}{\beta_{\text{pl}}}\left(\log \frac{\pi_{\theta}(\boldsymbol{a}^i |\boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})}\right) + \frac{1}{\beta_{\text{op}}} \left(\log \frac{\rho_{\phi}(\boldsymbol{a}^{-i} |\boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^{-i} |\boldsymbol{s})}\right) + \log\left(\frac{M_{\text{pl}}(\boldsymbol{s})M_{\text{op}}(\boldsymbol{s})}{N_{\text{pl}}(\boldsymbol{s})N_{\text{op}}(\boldsymbol{s})}\right) \Bigg]
        \end{aligned}
    \end{aligned}
\end{equation}
This is equivalent to maximizing the following objective
\begin{equation}
    \mathbb{E}\Bigg[ \mathcal{R}_{G}(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) - \frac{1}{\beta_{\text{pl}}}\left(\log \frac{\pi_{\theta}(\boldsymbol{a}^i |\boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^i | \boldsymbol{s})}\right) - \frac{1}{\beta_{\text{op}}} \left(\log \frac{\rho_{\phi}(\boldsymbol{a}^{-i} |\boldsymbol{s})}{P_{\text{prior}}(\boldsymbol{a}^{-i} |\boldsymbol{s})}\right) - \log\left(\frac{M_{\text{pl}}(\boldsymbol{s})M_{\text{op}}(\boldsymbol{s})}{N_{\text{pl}}(\boldsymbol{s})N_{\text{op}}(\boldsymbol{s})}\right) \Bigg]
\end{equation}
Noted that the expectation is based on the variational distribution, and thus the agent's policy and opponent's policy is based on the form of \ref{balance-pol-op}. Although the final equation is somewhat undesirable, this is an alternative to  Lagrange multiplier proposed in \cite{grau2018balancing}.

% \section{PR2 \cite{wen2019probabilistic}}
% For PR2, the graphical model is similar to \cite{tian2019regularized}, therefore, the derivation is similar. 

\section{ROMMEO \cite{tian2019regularized} x Balance Soft Q-Learning \cite{grau2018balancing}}

Now we can merge the idea of both model into one, whereby this will enable \cite{tian2019regularized} to be able to work within the zero-sum domain with known other agent's rationality and intention.

\section{Dealing with Unknown $\beta_{-i}$}
Now we would like to extend the merge of ROMMEO \cite{tian2019regularized} and Balance Soft Q-Learning \cite{grau2018balancing}, in which, the agent have some belief over the rationality and intention of other agent. This must be employed some inverse reinforcement learning in multi-agent domain, which is extensively studied. Furthermore, we would like to assert that  \cite{grau2018balancing} has ,also, done some work on estimating $\beta_{-i}$ via maximum likelihood.

\begin{figure}[ht]
    \begin{minipage}[t]{0.5\linewidth}
    \centering
    \begin{tikzpicture}[latent/.append style={minimum size=1.3cm}]
        \node[obs] (o) {$\mathcal{O}^i$};
        \node[latent, below=of o] (a) {$\boldsymbol{a}^i$};
        \node[latent, below=of a] (s) {$\boldsymbol{s}$};
        \node[latent, below left=1.3cm of s] (a1model) {$\boldsymbol{a}^{-i}_{\text{model}}$};
        
        \node[obs, right=2.3cm of a1model] (a1real) {$\boldsymbol{a}^{-i}_{\text{real}}$};
        \node [obs, above right=1.3cm of a] (beta)  {$\beta_i$};
        \node [latent, above right=1.5cm of a1real] (beta1)  {$\beta_{-i}$};
        \node[obs, below=of a1model] (o1) {$\mathcal{O}^{-i}$};
    
        \edge {s} {a, a1model, a1real}
        \edge {beta1} {a1model, a1real}
        \edge {beta} {a}
        \edge {a} {o}
        \edge {o1} {a1model}
        \path[->]  (a1model)  edge   [bend left=45] node {} (o);
        \path[->]  (s)  edge   [bend left=30] node {} (o);
        \path[->]  (o1)  edge   [bend left=60] node {} (o);
        \path[->]  (a1model)  edge   [bend left=45] node {} (a);
    \end{tikzpicture}
    \label{Balancing-Graphical}
    \end{minipage}%
    \begin{minipage}[t]{0.5\linewidth}
    We will model the belief over $\beta_{-i}$, which influences opponent's action, and we will use this value to construct perfect opponent model, such that agent can best response to. \Phu{It would be interesting if we assume that the real action of other agents based on our action (which isn't the case in one step game)}
    \end{minipage}
\end{figure}

\blindtext

\section{Probabilistic Model For General Sum Game}
Most of the models till now are all based on the centralized game rewards of all agents, while in \cite{grau2018balancing} the type of the game is based on the sign of $\beta_{-i}$, which reflects how the opponent agent itself react based on its perspective of the game reward.

This carves the way for us by defining $\beta_{-i}(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})$ to be varied according to all agents actions and state. \Phu{VERY VERY WEIRD INITIAL IDEA. This would be a disaster for defining the normalizing factor ?}, notably 
\begin{equation}
    \beta_{-i}(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i}) = \frac{R_{-i}(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})}{R_i(\boldsymbol{s}, \boldsymbol{a}^i, \boldsymbol{a}^{-i})}
\end{equation}