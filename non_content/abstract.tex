% In this thesis, we aimed to unify difference probabilistic multi-agent reinforcement learning algorithms into one framework that allows us to extend previous works to operate in more general cases. We first show that the current probabilistic modelling doesn't allow the zero-sum game to be solved, in which we proposed double probabilistic modelling that iterative update the agent and its opponent model with theoretical guarantee such as proof of convergence and opponent modeling improvement theorems. This is analogous to fictitious play, however, instead of best response to the opponent's policy, the proposed algorithm is best responding to opponent's optimal policy. After finalizing the unified framework for all algorithms, we then show that this framework can be extended into hierarchical multi-agent reinforcement learning and can be used to model belief of opponent's private information, thus proving the effectiveness and flexibility of the framework. We test our algorithm on challenging benchmarks against known algorithms with high-dimensional state description, proving empirically the performance of derived algorithms. We hope that framing multi-agent reinforcement learning into probabilistic inference problem will give rise to many novel algorithms, while being flexible to extends into known methods within probabilistic realm.


% In this thesis, we investigate a family of algorithms called maximum entropy multi-agent reinforcement learning (MEMARL), which is derived from variational inferences, and is a direct extension to a well-established control-as-inference framework in single agent reinforcement learning. By framing multi-agent problems as a probabilistic inference, one can includes concepts such as recursive reasoning \cite{wen2019probabilistic, wen2019multi} into a multi-agent reinforcement learning algorithms, while being decentralized training and decentralized execution. However, there are some limitation into such algorithms, notably inability to solve zero-sum game. In order to solve this major drawn back, we will have to step-back and derived an unified views of most algorithms in the literature. Our contributions are mainly as follows: 

% \begin{enumerate}
%     \item Deriving a tools that would let us constructs all the algorithms within MEMARL family of algorithms, which will show the flawed in some of MEMARL algorithms.
%     \item Given these tools, we provided a simple extension to some of MEMARL algorithm, so that they works within zero-sum game, with some theoretical guarantee.
%     \item We will also show that our algorithms works in practice with strong empirical results against various state-of-the-art algorithms.
%     \item After solving the major drawn-back within MEMARL framework, we will borrow techniques from control-as-inference for single agents framework to improve the currents multi-agent algorithms, for example, enable it to do hierarchical controls to represent belief over states, or to develop an algorithm based on information theoretic views of control. This would proof the flexibility of our developed views. 
%     % \item There still exists some algorithms that are not part of MEMARL frameworks, thus, we will try to provided the best mapping from these algorithms to our frameworks. \item Finally, we will investigate the effects on the behavior the algorithms derived from MEMARL framework, since there has been concern regarding the theoretical performance of current control-as-inference frameworks, especially, exploration. After understanding the drawn backs from control-as-inference frameworks, we will proposed a solutions that address the problems posted, and provide both theoretical guarantee and empirical results. \textbf{(MAYBE)}
% \end{enumerate}
% The main aim of this thesis is to provide tools necessary to systematically derived multi-agent reinforcement learning based on Baysian inference, and to resolve inconsistency within the literature, mainly the derivation of the algorithms. Furthermore, all of the algorithms presented are examined theoretically and empirically against current state-of-the-art.

% \Phu{I am aimed for 1-3 and some algorithm within 4. However, if there is a time there is a chance of finishing all of them + this would be a good list of publications}

In this thesis, we aim to gain more understanding of family called maximum entropy multi-agent reinforcement learning (MEMARL), which is based on well-known single agent reinforcement learning framework called maximum entropy reinforcement learning (MERL) or control as inference\footnote{We will use these terms interchangeably}. that utilized variational inference. By framing multi-agent problems as probabilistic inference, one can include concepts such as recursive reasoning \cite{wen2019probabilistic, wen2019multi} into a multi-agent reinforcement learning algorithms, while being decentralized training and decentralized execution. However, there are some drawbacks with this approach, notably, its inability to train agents that have a conflict of interests. In this work, we try to solve this drawback by investigating the algorithm thoroughly, which leads to valuable insight which is hidden in plain sight. Furthermore, we will also argue in support of MEMARL framework by showing that many complex forms of agent can be systematically derived/reinterpret using techniques in this framework. Ultimately, one might view this thesis as a \textit{cookbook} for deriving multi-agent reinforcement learning, which we hope to be useful in future research. Our contributions are listed as follows:
\begin{itemize}
    \item Examine the existing algorithms withing MEMARL framework, what make them successful, and what their flaws are. We will also consider another algorithm called Balancing-Q learning \cite{grau2018balancing}, which is closely related to this framework but lacks proper probabilistic interpretation. By re-deriving results, we can discover the flaws and thus able to mitigate some of the problems. 
    \item After fixing the problem of the framework, we shall move on to apply the framework to various multi-agent problems that have different characteristic, notably delayed communication and representing public belief. We will provide reinterpretations of some of the algorithms in the literature, proving the strength and versatility of the framework.
\end{itemize}
Finally, looking at the bigger picture, we would like to show one of the ways multi-agent reinforcement algorithms could be derived by providing tools and patterns that one can deploy. At the same time, examine/create a new algorithm. And, by using the framework that is based on a popular single-agent framework, we can apply some of the works done in single-agent literature into the multi-agent domain with relative ease. 