ELBO or Evidence Lower Bound Objective is one of the most interesting and fruitful objective functions. Its applications span in the from probabilistic generative models \cite{kingma2013auto} to reinforcement learning \cite{levine2018reinforcement}. This section will provides common "pattern" for deriving ELBO, which ultimately leads to Expectation Maximization algorithm, variational inference and variational auto-encoder \cite{kingma2013auto}  \Phu{Cite EM Cite VI add more examples}

Suppose we would like to maximize the probability of generating data point $\boldsymbol{X}$ given the parameter $\boldsymbol{\theta}$. We can assume there exists a latent variable $\boldsymbol{z}$ that generates this data point. Writing it as (assuming $q(\boldsymbol{z})$ is an arbitrary distribution over $\boldsymbol{z}$)

\begin{equation}
    \begin{aligned}
         \log P(\boldsymbol{X} | \boldsymbol{\theta}) &= \int P(\boldsymbol{X}, \boldsymbol{z} | \boldsymbol{\theta}) \ d\boldsymbol{z} \\
         &= \log \int P(\boldsymbol{X}, \boldsymbol{z}) \frac{q(\boldsymbol{z})}{q(\boldsymbol{z})} \ d \boldsymbol{z} \\
         &= \log \mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z})} \left[ \frac{P(\boldsymbol{X}, \boldsymbol{z} | \boldsymbol{\theta})}{q(\boldsymbol{z})} \right] \\
         &\ge \mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z})} \left[ \log\frac{P(\boldsymbol{X}, \boldsymbol{z} | \boldsymbol{\theta})}{q(\boldsymbol{z})} \right]
    \end{aligned}
\end{equation}
The inequality came from Jensen's inequality, hence we arrived at ELBO. The difference between ELBO and $\log P(\boldsymbol{X} |\boldsymbol{\theta})$ is equal to 

\begin{equation}
    \begin{aligned}
        \log P(\boldsymbol{X} |\boldsymbol{\theta}) - &\mathbb{E}_{\boldsymbol{z} \sim p(\boldsymbol{z})} \left[ \log\frac{P(\boldsymbol{X}, \boldsymbol{z} | \boldsymbol{\theta})}{q(\boldsymbol{z})} \right] \\
        &= \log P(\boldsymbol{X} |\boldsymbol{\theta}) - \mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z})}\big[ \log P(\boldsymbol{X}, \boldsymbol{z} | \boldsymbol{\theta}) \big] + \mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z})} \big[ \log q(\boldsymbol{z}) \big]  \\ 
        &= \cancel{\log P(\boldsymbol{X} |\boldsymbol{\theta})} - \mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z})}\big[ \log P( \boldsymbol{z} |\boldsymbol{X} , \boldsymbol{\theta}) \big] - \cancel{\mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z})}\big[ \log P( \boldsymbol{X} | \boldsymbol{\theta}) \big]} + \mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z})} \big[ \log q(\boldsymbol{z}) \big] \\ 
        &=  \mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z})} \big[ \log q(\boldsymbol{z}) \big] - \mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z})} \big[ \log P( \boldsymbol{z} |\boldsymbol{X} , \boldsymbol{\theta}) \big] \\
        &= D_{KL} \left( q(\boldsymbol{z}) || P( \boldsymbol{z} |\boldsymbol{X} , \boldsymbol{\theta}) \right)
    \end{aligned}
\end{equation}
This leads to first optimization step of EM algorithm (E-Step), where the goals of this algorithm is maximizes $\log$ probability. We would like to minimizing the "gap" between ELBO and $P(\boldsymbol{x} | \boldsymbol{\theta})$, therefore in E-Step, we set: 

$$
q(\boldsymbol{z}) \leftarrow P( \boldsymbol{z} |\boldsymbol{X} , \boldsymbol{\theta})
$$

So now, we have that ELBO is equal to $P(\boldsymbol{x} | \boldsymbol{\theta})$, which we now proceeded to maximizing ELBO based on $\theta$, which is equal to optimizing the following objective. 

\begin{equation}
    \arg\max_{\boldsymbol{\theta}} \mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z})}\big[\log P(\boldsymbol{X}, \boldsymbol{z} | \theta)\big]
\end{equation}
We call this step, M-Step. The limitation of EM algorithm lies within in E-Step, suppose, we can't even compute $P( \boldsymbol{z} |\boldsymbol{X} , \boldsymbol{\theta})$ then we have to minimizing the KL distance with respect to parameterized $\phi$ of $q(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\phi})$, instead: 

\begin{equation}
    D_{KL}(q(\boldsymbol{z}| \boldsymbol{X}, \boldsymbol{\phi}) || P( \boldsymbol{z} |\boldsymbol{X} , \boldsymbol{\theta}) )
\end{equation}
Which can be shown to be equal to minimizing ELBO for both parameters $\boldsymbol{\phi}, \boldsymbol{\theta}$
\begin{equation}
    \begin{aligned}
        D_{KL}(q(\boldsymbol{z}| \boldsymbol{X}, \boldsymbol{\phi}) || P( \boldsymbol{z} |\boldsymbol{X} , \boldsymbol{\theta}) ) &= \mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\phi})} \left[ \log \frac{q(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\phi})}{P(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\theta})} \right] \quad \text{ where } P(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\theta}) = \frac{P(\boldsymbol{X} | \boldsymbol{z}, \boldsymbol{\theta}) P(\boldsymbol{z})}{P(\boldsymbol{X} | \boldsymbol{\theta})} \\ 
        &= \mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\phi})} \left[ \log \frac{q(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\phi}) P(\boldsymbol{X} | \boldsymbol{\theta})}{P(\boldsymbol{X} | \boldsymbol{z}, \boldsymbol{\theta}) P(\boldsymbol{z})} \right] \\ 
        &= \underbrace{\mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\phi})} \left[ \log \frac{q(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\phi})}{P(\boldsymbol{X} | \boldsymbol{z}, \boldsymbol{\theta}) P(\boldsymbol{z})} \right]}_{\text{ELBO}} + \mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\phi})} \left[ \log P(\boldsymbol{X} | \boldsymbol{\theta}) \right]  \\ 
        &= -\mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\phi})} \left[ \log P(\boldsymbol{X} | \boldsymbol{z}, \boldsymbol{\theta}) \right] + \mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\phi})} \left[ \log \frac{q(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\phi})}{P(\boldsymbol{z})} \right] + \text{const} \\ 
        &= -\mathbb{E}_{\boldsymbol{z} \sim q(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\phi})} \left[ \log P(\boldsymbol{X} | \boldsymbol{z}, \boldsymbol{\theta}) \right] + D_{KL}(q(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\phi}) || P(\boldsymbol{z})) + \text{const}
    \end{aligned}
\end{equation}
Now we can also arrived at the ELBO objective. We can interpreted optimizing ELBO with respect to $\phi, \theta$ as EM algorithm where optimizing with respect to $\phi$ is an E-Step and optimizing with respect to $\theta$ is an M-Step. This is also used as optimizing objective in Variational Auto Encoder \cite{kingma2013auto}, where we denote $q(\boldsymbol{z} | \boldsymbol{X}, \boldsymbol{\phi})$ as an encoder and $P(\boldsymbol{X} | \boldsymbol{z}, \boldsymbol{\theta})$ as a decoder


