In the survey \cite{levine2018reinforcement}, the authors provided probabilistic interpretation of reinforcement learning, however, they assume the prior over actions to be uniform. Similarly \cite{grau2018soft} briefly provide and derive objective with explicit prior over actions. In this section, we will go in depth on how to derived such an objective.

We start with assuming a hidden variables $\boldsymbol{a}_{1:T}$ and $\boldsymbol{s}_{1:T}$, where the observed variable to be the optimality $\mathcal{O}_{1:T}$. Given by the following graphical models.

\begin{figure}[!h]
  \tikz{
     \node[obs] (o1) {$\mathcal{O}_1$};
     \node[latent, below=of o1] (a1) {$\bold{a}_1$};
     \node[latent, below=of a1] (s1) {$\bold{s}_1$};
     
     \node[obs, right=of o1] (o2) {$\mathcal{O}_2$};
     \node[latent, right=of a1] (a2) {$\bold{a}_2$};
     \node[latent, right=of s1] (s2) {$\bold{s}_2$};
     
     \node[latent, right=of s2] (s3) {$\bold{s}_3$};
     \node[latent, right=of s3] (st) {$\bold{s}_T$};
     \node at ($(s3)!.5!(st)$) {\ldots};
     
     \path[->]  (s1)  edge   [bend left=30] node {} (o1);
     \edge {a1} {o1}  
     
     \path[->]  (s2)  edge   [bend left=30] node {} (o2);
     \edge {s1, a1} {s2}
     \edge {a2} {o2}
     
     \edge {s2, a2} {s3}
    %  \node[latent,above=of o, xshift=-1cm,fill] (y) {$y$}; 
    %  \node[latent,above=of o,xshift=1cm] (z) {$z$};
    %  \edge {y,z} {o}  
 }
\end{figure}

\Phu{Check this out}
Where we defined the optimality $\mathcal{O}_{1:T}$ to be (Assuming $r$ to always be positive)
\begin{equation}
    P(\mathcal{O}_{t} = 1 | \boldsymbol{s}_{t}, \boldsymbol{a}_{t}) = \frac{\exp\left(\beta \cdot r(\boldsymbol{s}_{t}, \boldsymbol{a}_{t})\right)}{\int \exp(\beta \cdot r(\boldsymbol{s}_{t}, \boldsymbol{a}_{t})) \ d\boldsymbol{s}_{t} d\boldsymbol{a}_{t}}
\end{equation}
And we can see that 
\begin{equation}
    P(\mathcal{O}_{1:T} | \boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \propto \exp \left( \beta \sum^T_{t=1} r(s_t, a_t) \right)
\end{equation}
Since each optimality variables are conditional independent for each time step. 

We would like to find approximated posterior distribution $q$, which we defined it by probabilistic graphical model as
\begin{figure}[!h]
  \tikz{
     \node[latent] (a1) {$\bold{a}_1$};
     \node[latent, below=of a1] (s1) {$\bold{s}_1$};
     
     \node[latent, right=of a1] (a2) {$\bold{a}_2$};
     \node[latent, right=of s1] (s2) {$\bold{s}_2$};
     
     \node[latent, right=of s2] (s3) {$\bold{s}_3$};
     \node[latent, right=of s3] (st) {$\bold{s}_T$};
     \node at ($(s3)!.5!(st)$) {\ldots};
     
     \edge {s1} {a1}  
     \edge {s1, a1} {s2}
     \edge {s2} {a2}
     
     \edge {s2, a2} {s3}
    %  \node[latent,above=of o, xshift=-1cm,fill] (y) {$y$}; 
    %  \node[latent,above=of o,xshift=1cm] (z) {$z$};
    %  \edge {y,z} {o}  
 }
\end{figure}

The joint probability distribution of $q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T})$ to be 
\begin{equation}
    q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) = P(\boldsymbol{s}_1)\prod^T_{t=1} P(\boldsymbol{s}_{t+1} | \boldsymbol{s}_t, \boldsymbol{a}_t) \pi_{\theta}(\boldsymbol{a}_t | \boldsymbol{s}_t)
\end{equation}
Now we can minimize the KL-Divergence to perform approximate variational inference.
\begin{equation}
        D_{KL} \Big[ q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \Big\lvert\Big\rvert P(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T} | \mathcal{O}_{1:T} = 1) \Big] = D_{KL} \left[ q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \Bigg\lvert\Bigg\rvert \frac{P(\mathcal{O}_{1:T} | \boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) P_{\text{prior}} (\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) }{P(\mathcal{O}_{1:T})} \right]
\end{equation}
The $P_{\text{prior}}$ is defined to be 
\begin{equation}
    P_{\text{prior}}(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) = P(\boldsymbol{s}_1)\prod^T_{t=1} P(\boldsymbol{s}_{t+1} | \boldsymbol{s}_t, \boldsymbol{a}_t) \pi_{\text{prior}}(\boldsymbol{a}_t | \boldsymbol{s}_t)
\end{equation}
Which the evaluation of KL-Divergence yields optimizing the following objective
\begin{equation}
    \begin{aligned}
        \mathbb{E}_{(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \sim q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T})} &\left[ \log q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \right] -  \mathbb{E}_{(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \sim q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T})} \left[ \log \frac{P(\mathcal{O}_{1:T} | \boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) P_{\text{prior}} (\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) }{P(\mathcal{O}_{1:T})} \right] \\ 
        &= \ \begin{aligned}[t]
           &\mathbb{E}_{(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \sim q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T})} \left[ \log \left[ P(\boldsymbol{s}_1) \prod^T_{t=1} P(\boldsymbol{s}_{t+1} | \boldsymbol{s}_t, \boldsymbol{a}_t) \pi_{\theta}(\boldsymbol{a}_t | \boldsymbol{s}_t) \right] \right] \\
            &- \mathbb{E}_{(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \sim q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T})} \big[ \log P(\mathcal{O}_{1:T} | \boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \big] \\ 
            &- \mathbb{E}_{(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \sim q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T})} \left[ \log \left[ P(\boldsymbol{s}_1) \prod^T_{t=1} P(\boldsymbol{s}_{t+1} | \boldsymbol{s}_t, \boldsymbol{a}_t) \pi_{\text{prior}}(\boldsymbol{a}_t | \boldsymbol{s}_t) \right] \right] 
        \end{aligned} \\
        &= \begin{aligned}[t]
            &\mathbb{E}_{(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \sim q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T})} \left[ \log P(\boldsymbol{s}_1) + \sum^T_{t=1} \log P(\boldsymbol{s}_{t+1} | \boldsymbol{s}_t, \boldsymbol{a}_t) + \log \pi_{\theta}(\boldsymbol{a}_t | \boldsymbol{s}_t) \right] \\
            &- \mathbb{E}_{(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \sim q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T})} \left[ \lambda \sum^T_{t=1} r(\boldsymbol{s}_t, \boldsymbol{a}_t) \right] \\
            &- \mathbb{E}_{(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \sim q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T})} \left[ \log P(\boldsymbol{s}_1) + \sum^T_{t=1} \log P(\boldsymbol{s}_{t+1} | \boldsymbol{s}_t, \boldsymbol{a}_t) + \log \pi_{\text{prior}}(\boldsymbol{a}_t | \boldsymbol{s}_t) \right]
        \end{aligned} \\ 
        &= \begin{aligned}[t]
            &-\mathbb{E}_{(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \sim q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T})} \left[\lambda \sum^T_{t=1}  r(\boldsymbol{s}_t, \boldsymbol{a}_t) \right] \\
            &+ \mathbb{E}_{(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \sim q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T})} \left[ \sum^T_{t=1} \log \pi_{\theta}(\boldsymbol{a}_t | \boldsymbol{s}_t) - \log \pi_{\text{prior}}(\boldsymbol{a}_t | \boldsymbol{s}_t) \right]
        \end{aligned}
    \end{aligned}
\end{equation}

The object we have maximizes becomes:
\begin{equation}
    \lambda \mathbb{E}_{(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T}) \sim q(\boldsymbol{s}_{1:T}, \boldsymbol{a}_{1:T})} \left[\sum^T_{t=1}  r(\boldsymbol{s}_t, \boldsymbol{a}_t) \right] - \sum^T_{t=1} D_{KL}\Big[ \pi_{\theta} (\boldsymbol{a}_t | \boldsymbol{s}_t) \Big|\Big| \pi_{\text{prior}} (\boldsymbol{a}_t | \boldsymbol{s}_t) \Big]
\end{equation}
Noted that if the $\pi_{\text{prior}}$ is uniform distribution, then the objective becomes maximum entropy reinforcement learning.

